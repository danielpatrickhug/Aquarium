{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "Knowledge_Distilliation_MNIST.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielpatrickhug/Aquarium/blob/main/Knowledge_Distilliation_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yr9V_t-3VfN"
      },
      "source": [
        "# Knowledge Distillation\n",
        "Reference: [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26wicJRA3VfO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjRLoK6x3VfO"
      },
      "source": [
        "## Teacher Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUKtnrNN3VfP"
      },
      "source": [
        "batch_size_teacher = 128\n",
        "test_batch_size_teacher= 1000\n",
        "epochs_teacher = 6\n",
        "epochs_student = 3\n",
        "lr_teacher =0.01\n",
        "momentum_teacher = 0.9\n",
        "seed = 42\n",
        "log_interval = 10\n",
        "no_cuda = False"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FnEI9Io3VfP"
      },
      "source": [
        "cuda = not no_cuda and torch.cuda.is_available()\n",
        "use_cuda= True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgFQ5EPD3VfP"
      },
      "source": [
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP2-woN83VfQ"
      },
      "source": [
        "train_kwargs = {'batch_size': batch_size_teacher}\n",
        "test_kwargs = {'batch_size': test_batch_size_teacher}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFg3UQEf3VfQ"
      },
      "source": [
        "if use_cuda:\n",
        "    cuda_kwargs = {'num_workers': 1,\n",
        "                    'pin_memory': True,\n",
        "                    'shuffle': True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5UNwZI73VfR"
      },
      "source": [
        "%%capture\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                    transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOGn26Oz3VfR"
      },
      "source": [
        "## Teacher Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKj5WqnI3VfS"
      },
      "source": [
        "class Teacher(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Teacher, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 1200)\n",
        "        self.fc2 = nn.Linear(1200, 1200)\n",
        "        self.fc3 = nn.Linear(1200, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, p=0.8)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.dropout(x, p=0.8)\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLLWr7d83VfS"
      },
      "source": [
        "def teacher_train(epoch, model):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "        "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNSeIDYm3VfT"
      },
      "source": [
        "def teacher_train_evaluate(model):\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        train_loss += F.cross_entropy(output, target).item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        train_loss, correct, len(train_loader.dataset),\n",
        "        100. * correct / len(train_loader.dataset)))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-svYbZG3VfT"
      },
      "source": [
        "def teacher_test(model):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        test_loss += F.cross_entropy(output, target).item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkORZ08v3VfT"
      },
      "source": [
        "## Teacher Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLtzneKC3VfU",
        "outputId": "dde067bd-bda6-4755-a7b5-cc35d63e9a9a"
      },
      "source": [
        "teacher_model = Teacher()\n",
        "teacher_model.to(device)\n",
        "optimizer = optim.SGD(teacher_model.parameters(), lr=lr_teacher, momentum=0.5,\n",
        "                      weight_decay=5e-4)\n",
        "for epoch in range(1, epochs_teacher + 1):\n",
        "    teacher_train(epoch, teacher_model)\n",
        "    teacher_train_evaluate(teacher_model)\n",
        "    teacher_test(teacher_model)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.400108\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.364313\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.309432\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.110107\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.063204\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.042824\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.826525\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.827687\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.638777\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.509249\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.415824\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.427236\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.304447\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.256538\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.241844\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.093959\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.217756\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.104575\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.002648\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.032651\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.965709\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.886998\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.850393\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.993640\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.971938\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.825616\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.769692\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.649613\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.751870\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.787392\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.712016\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.731372\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.736722\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.695990\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.769989\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.662890\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.759479\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.709996\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.739762\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.577209\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.700250\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.633793\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.649093\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.788278\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.775248\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.648124\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.537344\n",
            "\n",
            "Train set: Average loss: 291.7858, Accuracy: 48083/60000 (80%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0006, Accuracy: 8103/10000 (81%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.535378\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.588573\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.553821\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.531175\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.628233\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.475600\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.455168\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.633364\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.527000\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.602606\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.674760\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.542045\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.434407\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.436863\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.621674\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.423474\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.533778\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.511967\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.441900\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.397514\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.513034\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.494095\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.599262\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.563340\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.529034\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.492884\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.639214\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.673504\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.552533\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.583776\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.419816\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.399067\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.446988\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.485658\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.567689\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.510865\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.466572\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.439829\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.519072\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.464962\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.451062\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.561587\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.610026\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.423507\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.349606\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.289735\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.487807\n",
            "\n",
            "Train set: Average loss: 215.0247, Accuracy: 51559/60000 (86%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0004, Accuracy: 8642/10000 (86%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.384415\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.451524\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.439702\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.602810\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.448781\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.396877\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.392230\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.531323\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.561271\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.287249\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.344727\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.468531\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.359755\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.396254\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.328046\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.397490\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.289541\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.384910\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.488751\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.610688\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.404241\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.523897\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.489507\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.451253\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.506528\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.295223\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.326017\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.377724\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.344397\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.405847\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.413440\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.590535\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.537139\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.392331\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.374312\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.522481\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.307916\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.340544\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.360971\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.371553\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.372319\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.327835\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.311419\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.347091\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.322983\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.391074\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.307583\n",
            "\n",
            "Train set: Average loss: 180.0360, Accuracy: 52979/60000 (88%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0004, Accuracy: 8869/10000 (89%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.403298\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.424352\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.503400\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.388755\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.483073\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.466897\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.321932\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.478351\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.320059\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.433319\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.339240\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.303271\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.360006\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.505306\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.536143\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.483707\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.403001\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.306425\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.413187\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.350801\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.296420\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.296862\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.361026\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.270790\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.367419\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.413324\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.444794\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.346054\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.381491\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.341819\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.373490\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.376959\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.269966\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.410944\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.382890\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.337104\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.337254\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.263225\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.303102\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.247890\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.374260\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.264769\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.439265\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.262259\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.357302\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.394095\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.350763\n",
            "\n",
            "Train set: Average loss: 160.2378, Accuracy: 53829/60000 (90%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0003, Accuracy: 8967/10000 (90%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.274368\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.226826\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.321407\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.257573\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.397982\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.428895\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.365454\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.247134\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.401328\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.301242\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.353275\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.268256\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.228524\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.402981\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.290841\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.290013\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.366753\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.314173\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.324149\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.326702\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.362034\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.330379\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.349763\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.338587\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.385928\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.201683\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.256275\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.342795\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.297112\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.325950\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.354858\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.221893\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.322092\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.233230\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.292921\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.388347\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.392358\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.230904\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.321729\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.322042\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.332029\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.226487\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.281550\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.314272\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.344483\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.382289\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.312008\n",
            "\n",
            "Train set: Average loss: 142.6872, Accuracy: 54533/60000 (91%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0003, Accuracy: 9097/10000 (91%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.268978\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.363311\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.186753\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.355566\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.371702\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.302093\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.337206\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.300464\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.329836\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.354499\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.364332\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.234653\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.206341\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.247358\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.157085\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.247491\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.400781\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.392127\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.298050\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.268718\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.259500\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.296942\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.330890\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.388164\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.324669\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.341246\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.226390\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.306965\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.223530\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.281208\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.234369\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.329153\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.257560\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.215758\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.352755\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.353839\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.221293\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.294940\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.321462\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.143421\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.189897\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.281706\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.304382\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.205875\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.248875\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.375528\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.291474\n",
            "\n",
            "Train set: Average loss: 131.8912, Accuracy: 54991/60000 (92%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0003, Accuracy: 9140/10000 (91%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK9CR9l93l7F"
      },
      "source": [
        "## Student Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upqX7Rq63VfU"
      },
      "source": [
        "class Student(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Student, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 800)\n",
        "        self.fc2 = nn.Linear(800, 800)\n",
        "        self.fc3 = nn.Linear(800, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_fiKAyR3pqh"
      },
      "source": [
        "## Distillation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIt-eSrZ3VfU"
      },
      "source": [
        "def distillation(y, labels, teacher_scores, T, alpha):\n",
        "    return nn.KLDivLoss()(F.log_softmax(y/T), F.softmax(teacher_scores/T)) * (T*T * 2.0 * alpha) + F.cross_entropy(y, labels) * (1. - alpha)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzJM-abk3VfV"
      },
      "source": [
        "def train(epoch, model, loss_fn):\n",
        "    model.train()\n",
        "    teacher_model.eval()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        teacher_output = teacher_model(data)\n",
        "        teacher_output = teacher_output.detach()\n",
        "        # teacher_output = Variable(teacher_output.data, requires_grad=False) #alternative approach to load teacher_output\n",
        "        loss = loss_fn(output, target, teacher_output, T=20.0, alpha=0.7)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyF6uDyk3VfV"
      },
      "source": [
        "def train_evaluate(model):\n",
        "    model.eval()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        train_loss += F.cross_entropy(output, target).item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        train_loss, correct, len(train_loader.dataset),\n",
        "        100. * correct / len(train_loader.dataset)))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kN6aZMP3VfW"
      },
      "source": [
        "def test(model):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        # test_loss += F.cross_entropy(output, target).data[0] # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-FaS2Sw3VfW"
      },
      "source": [
        "model = Student()\n",
        "model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f30IC7kX3tFe"
      },
      "source": [
        "## Student Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFIdHN7A3VfW",
        "outputId": "49479a9f-056f-41e6-c64c-e613b0291845"
      },
      "source": [
        "for epoch in range(1, epochs_student + 1):\n",
        "    train(epoch, model, loss_fn=distillation)\n",
        "    train_evaluate(model)\n",
        "    test(model)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.121444\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.985574\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.876781\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.898033\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.732158\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.849338\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.687862\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.801792\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.597332\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.562558\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.405815\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.275338\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.132553\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.112868\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.886485\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.889022\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.794079\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.788738\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.732066\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.736556\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.693714\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.595899\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.599745\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.559143\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.622738\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.572857\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.526220\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.509265\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.484752\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.480113\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.438598\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.468911\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.480392\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.477965\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.413535\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.479252\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.412399\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.422089\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.384488\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.407965\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.452538\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.406365\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.426767\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.456129\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.446401\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.440717\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.396541\n",
            "\n",
            "Train set: Average loss: 184.1954, Accuracy: 53028/60000 (88%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 8872/10000 (89%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.410840\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.444949\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.415030\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.420441\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.457186\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.391579\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.368746\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.378342\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.385223\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.406112\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.353489\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.358675\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.357314\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.338548\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.338853\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.316884\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.326941\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.377184\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.358862\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.377938\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.329466\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.376106\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.360538\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.327162\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.318878\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.364612\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.370542\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.371944\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.303000\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.334732\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.316512\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.350529\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.363992\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.321143\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.348295\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.326180\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.315940\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.359185\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.322478\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.310270\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.350435\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.297395\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.317050\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.370554\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.373801\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.358467\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.372158\n",
            "\n",
            "Train set: Average loss: 146.6113, Accuracy: 54357/60000 (91%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9086/10000 (91%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.317525\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.310013\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.320954\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.301685\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.282315\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.354706\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.309962\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.343209\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.279577\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.374165\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.363750\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.311040\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.332518\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.290049\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.314290\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.342985\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.345251\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.331601\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.294927\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.324269\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.304506\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.334448\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.287375\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.229848\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.363786\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.295011\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.309627\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.328976\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.320224\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.309937\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.302577\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.350780\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.329948\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.307658\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.312971\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.366530\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.354409\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.320967\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.267456\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.295650\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.311942\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.283752\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.351070\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.278339\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.307966\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.305443\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.269145\n",
            "\n",
            "Train set: Average loss: 129.0581, Accuracy: 54988/60000 (92%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9214/10000 (92%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}