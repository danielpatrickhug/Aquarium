{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT3_and_Chill.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yu3_xbxXSlVD",
        "QWi6OdgZoLpm"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkDI1kN4O4r4"
      },
      "source": [
        "# GPT Inference\n",
        "- Using https://github.com/EleutherAI/gpt-neo open source implementation of GPT-3 2.7B model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYxiIOyqPcql"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAOMe8mhOy0z"
      },
      "source": [
        "%%capture\n",
        "%tensorflow_version 2.x\n",
        "!git clone https://github.com/EleutherAI/GPTNeo\n",
        "%cd GPTNeo\n",
        "!pip3 install -q -r requirements.txt\n",
        "pretrained_model = None\n",
        "dataset = None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCwd64eAQIAh"
      },
      "source": [
        "# GCP Setup "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPGsqR3kPZg_",
        "outputId": "d1ebebfd-9e21-4d8f-95a7-0b643ec8e5c7"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome! This command will take you through the configuration of gcloud.\n",
            "\n",
            "Settings from your current configuration [default] are:\n",
            "component_manager:\n",
            "  disable_update_check: 'True'\n",
            "compute:\n",
            "  gce_metadata_read_timeout_sec: '0'\n",
            "core:\n",
            "  account: novelspaceonly@gmail.com\n",
            "\n",
            "Pick configuration to use:\n",
            " [1] Re-initialize this configuration [default] with new settings \n",
            " [2] Create a new configuration\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "Your current configuration has been set to: [default]\n",
            "\n",
            "You can skip diagnostics next time by using the following flag:\n",
            "  gcloud init --skip-diagnostics\n",
            "\n",
            "Network diagnostic detects and fixes local network connection issues.\n",
            "Reachability Check passed.\n",
            "Network diagnostic passed (1/1 checks passed).\n",
            "\n",
            "Choose the account you would like to use to perform operations for \n",
            "this configuration:\n",
            " [1] novelspaceonly@gmail.com\n",
            " [2] Log in with a new account\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "You are logged in as: [novelspaceonly@gmail.com].\n",
            "\n",
            "Pick cloud project to use: \n",
            " [1] cool-encoder-306021\n",
            " [2] gpt-neo-chill\n",
            " [3] Create a new project\n",
            "Please enter numeric choice or text value (must exactly match list \n",
            "item):  2\n",
            "\n",
            "Your current project has been set to: [gpt-neo-chill].\n",
            "\n",
            "Not setting default zone/region (this feature makes it easier to use\n",
            "[gcloud compute] by setting an appropriate default value for the\n",
            "--zone and --region flag).\n",
            "See https://cloud.google.com/compute/docs/gcloud-compute section on how to set\n",
            "default compute region and zone manually. If you would like [gcloud init] to be\n",
            "able to do this for you the next time you run it, make sure the\n",
            "Compute Engine API is enabled for your project on the\n",
            "https://console.developers.google.com/apis page.\n",
            "\n",
            "Your Google Cloud SDK is configured and ready to use!\n",
            "\n",
            "* Commands that require authentication will use novelspaceonly@gmail.com by default\n",
            "* Commands will reference project `gpt-neo-chill` by default\n",
            "Run `gcloud help config` to learn how to change individual settings\n",
            "\n",
            "This gcloud configuration is called [default]. You can create additional configurations if you work with multiple accounts and/or projects.\n",
            "Run `gcloud topic configurations` to learn more.\n",
            "\n",
            "Some things to try next:\n",
            "\n",
            "* Run `gcloud --help` to see the Cloud Platform services you can interact with. And run `gcloud help COMMAND` to get help on any gcloud command.\n",
            "* Run `gcloud topic --help` to learn about advanced features of the SDK like arg files and output formatting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSKmkyRIQlEi"
      },
      "source": [
        "path_to_cloud_bucket = 'gs://gpt-neo-chill/' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu3_xbxXSlVD"
      },
      "source": [
        "# Download pretrained model weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idAyuCgqRrPL",
        "outputId": "8e24ca36-696b-45c9-c02f-abec9cdb9581"
      },
      "source": [
        "\n",
        "pretrained_model = 'GPT3_2-7B' #@param [\"GPT3_XL\", \"GPT3_2-7B\"]\n",
        "!wget -m -np -c -U \"eye02\" -w 2 -R \"index.html*\" \"https://the-eye.eu/public/AI/gptneo-release/$pretrained_model/\"\n",
        "path_to_local_weights = f\"/content/GPTNeo/the-eye.eu/public/AI/gptneo-release/{pretrained_model}\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-28 20:15:57--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/\n",
            "Resolving the-eye.eu (the-eye.eu)... 162.213.130.242\n",
            "Connecting to the-eye.eu (the-eye.eu)|162.213.130.242|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/index.html.tmp’\n",
            "\n",
            "\r          the-eye.e     [<=>                 ]       0  --.-KB/s               \rthe-eye.eu/public/A     [ <=>                ]  14.42K  --.-KB/s    in 0.03s   \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2021-03-28 20:15:57 (462 KB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/index.html.tmp’ saved [14764]\n",
            "\n",
            "Loading robots.txt; please ignore errors.\n",
            "--2021-03-28 20:15:59--  https://the-eye.eu/robots.txt\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4 [text/plain]\n",
            "Saving to: ‘the-eye.eu/robots.txt’\n",
            "\n",
            "the-eye.eu/robots.t 100%[===================>]       4  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-28 20:15:59 (1.37 MB/s) - ‘the-eye.eu/robots.txt’ saved [4/4]\n",
            "\n",
            "Removing the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/index.html.tmp since it should be rejected.\n",
            "\n",
            "--2021-03-28 20:16:01--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/checkpoint\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 523 [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/checkpoint’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>]     523  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-28 20:16:01 (176 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/checkpoint’ saved [523/523]\n",
            "\n",
            "--2021-03-28 20:16:03--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/config.json\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 937 [application/json]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/config.json’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>]     937  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-28 20:16:03 (174 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/config.json’ saved [937/937]\n",
            "\n",
            "--2021-03-28 20:16:05--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00000-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8 [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00000-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>]       8  --.-KB/s    in 0s      \n",
            "\n",
            "2021-03-28 20:16:05 (2.90 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00000-of-00064’ saved [8/8]\n",
            "\n",
            "--2021-03-28 20:16:07--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00001-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 571514880 (545M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00001-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 545.04M  85.7MB/s    in 6.6s    \n",
            "\n",
            "2021-03-28 20:16:14 (82.6 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00001-of-00064’ saved [571514880/571514880]\n",
            "\n",
            "--2021-03-28 20:16:16--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00002-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 514631680 (491M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00002-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 490.79M  42.5MB/s    in 8.5s    \n",
            "\n",
            "2021-03-28 20:16:24 (57.6 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00002-of-00064’ saved [514631680/514631680]\n",
            "\n",
            "--2021-03-28 20:16:26--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00003-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 466841600 (445M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00003-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 445.21M  23.0MB/s    in 16s     \n",
            "\n",
            "2021-03-28 20:16:42 (28.1 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00003-of-00064’ saved [466841600/466841600]\n",
            "\n",
            "--2021-03-28 20:16:44--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00004-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 524410880 (500M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00004-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 500.12M  21.3MB/s    in 30s     \n",
            "\n",
            "2021-03-28 20:17:14 (16.7 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00004-of-00064’ saved [524410880/524410880]\n",
            "\n",
            "--2021-03-28 20:17:16--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00005-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 466841600 (445M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00005-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 445.21M  25.2MB/s    in 17s     \n",
            "\n",
            "2021-03-28 20:17:34 (25.9 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00005-of-00064’ saved [466841600/466841600]\n",
            "\n",
            "--2021-03-28 20:17:36--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00006-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 550676480 (525M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00006-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 525.17M  22.3MB/s    in 31s     \n",
            "\n",
            "2021-03-28 20:18:07 (16.7 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00006-of-00064’ saved [550676480/550676480]\n",
            "\n",
            "--2021-03-28 20:18:09--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00007-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 471869440 (450M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00007-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 450.01M  33.7MB/s    in 23s     \n",
            "\n",
            "2021-03-28 20:18:32 (19.5 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00007-of-00064’ saved [471869440/471869440]\n",
            "\n",
            "--2021-03-28 20:18:34--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00008-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 498227200 (475M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00008-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 475.15M  20.8MB/s    in 17s     \n",
            "\n",
            "2021-03-28 20:18:52 (27.2 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00008-of-00064’ saved [498227200/498227200]\n",
            "\n",
            "--2021-03-28 20:18:54--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00009-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 550676480 (525M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00009-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 525.17M  22.8MB/s    in 24s     \n",
            "\n",
            "2021-03-28 20:19:18 (21.8 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00009-of-00064’ saved [550676480/550676480]\n",
            "\n",
            "--2021-03-28 20:19:20--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00010-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 645734400 (616M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00010-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 615.82M  15.7MB/s    in 36s     \n",
            "\n",
            "2021-03-28 20:19:56 (17.1 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00010-of-00064’ saved [645734400/645734400]\n",
            "\n",
            "--2021-03-28 20:19:58--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00011-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 472074240 (450M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00011-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 450.21M  26.0MB/s    in 32s     \n",
            "\n",
            "2021-03-28 20:20:30 (14.1 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00011-of-00064’ saved [472074240/472074240]\n",
            "\n",
            "--2021-03-28 20:20:32--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00012-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 498227200 (475M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00012-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 475.15M  18.8MB/s    in 17s     \n",
            "\n",
            "2021-03-28 20:20:49 (27.7 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00012-of-00064’ saved [498227200/498227200]\n",
            "\n",
            "--2021-03-28 20:20:51--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00013-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 550799360 (525M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00013-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 525.28M  13.3MB/s    in 33s     \n",
            "\n",
            "2021-03-28 20:21:24 (16.0 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00013-of-00064’ saved [550799360/550799360]\n",
            "\n",
            "--2021-03-28 20:21:26--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00014-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 550563840 (525M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00014-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 525.06M  18.7MB/s    in 35s     \n",
            "\n",
            "2021-03-28 20:22:01 (14.9 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00014-of-00064’ saved [550563840/550563840]\n",
            "\n",
            "--2021-03-28 20:22:03--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00015-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 498339840 (475M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00015-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 475.25M  14.6MB/s    in 30s     \n",
            "\n",
            "2021-03-28 20:22:33 (16.1 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00015-of-00064’ saved [498339840/498339840]\n",
            "\n",
            "--2021-03-28 20:22:35--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00016-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 550799360 (525M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00016-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 525.28M  18.5MB/s    in 28s     \n",
            "\n",
            "2021-03-28 20:23:03 (18.8 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00016-of-00064’ saved [550799360/550799360]\n",
            "\n",
            "--2021-03-28 20:23:05--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00017-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 471961600 (450M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00017-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 450.10M  18.1MB/s    in 24s     \n",
            "\n",
            "2021-03-28 20:23:29 (18.8 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00017-of-00064’ saved [471961600/471961600]\n",
            "\n",
            "--2021-03-28 20:23:31--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00018-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 472064000 (450M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00018-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 450.20M  7.98MB/s    in 43s     \n",
            "\n",
            "2021-03-28 20:24:14 (10.5 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00018-of-00064’ saved [472064000/472064000]\n",
            "\n",
            "--2021-03-28 20:24:16--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00019-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 472064000 (450M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00019-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 450.20M  23.9MB/s    in 24s     \n",
            "\n",
            "2021-03-28 20:24:40 (18.6 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00019-of-00064’ saved [472064000/472064000]\n",
            "\n",
            "--2021-03-28 20:24:42--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00020-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 472043520 (450M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00020-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 450.18M  48.7MB/s    in 12s     \n",
            "\n",
            "2021-03-28 20:24:54 (37.8 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00020-of-00064’ saved [472043520/472043520]\n",
            "\n",
            "--2021-03-28 20:24:56--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00021-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 550584320 (525M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00021-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 525.08M  27.2MB/s    in 14s     \n",
            "\n",
            "2021-03-28 20:25:10 (38.5 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00021-of-00064’ saved [550584320/550584320]\n",
            "\n",
            "--2021-03-28 20:25:12--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00022-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 550584320 (525M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00022-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 525.08M  9.80MB/s    in 28s     \n",
            "\n",
            "2021-03-28 20:25:40 (18.9 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00022-of-00064’ saved [550584320/550584320]\n",
            "\n",
            "--2021-03-28 20:25:42--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00023-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 472043520 (450M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00023-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 450.18M  17.0MB/s    in 35s     \n",
            "\n",
            "2021-03-28 20:26:17 (12.9 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00023-of-00064’ saved [472043520/472043520]\n",
            "\n",
            "--2021-03-28 20:26:19--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00024-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 471992320 (450M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00024-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 450.13M  24.7MB/s    in 27s     \n",
            "\n",
            "2021-03-28 20:26:46 (16.5 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00024-of-00064’ saved [471992320/471992320]\n",
            "\n",
            "--2021-03-28 20:26:48--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00025-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 471961600 (450M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00025-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 450.10M  14.6MB/s    in 35s     \n",
            "\n",
            "2021-03-28 20:27:23 (13.0 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00025-of-00064’ saved [471961600/471961600]\n",
            "\n",
            "--2021-03-28 20:27:25--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00026-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 550666240 (525M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00026-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 525.16M  17.3MB/s    in 37s     \n",
            "\n",
            "2021-03-28 20:28:02 (14.4 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00026-of-00064’ saved [550666240/550666240]\n",
            "\n",
            "--2021-03-28 20:28:04--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00027-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 645734400 (616M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00027-of-00064’\n",
            "\n",
            "the-eye.eu/public/A 100%[===================>] 615.82M  22.7MB/s    in 36s     \n",
            "\n",
            "2021-03-28 20:28:40 (17.0 MB/s) - ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00027-of-00064’ saved [645734400/645734400]\n",
            "\n",
            "--2021-03-28 20:28:42--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00028-of-00064\n",
            "Reusing existing connection to the-eye.eu:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 498339840 (475M) [application/octet-stream]\n",
            "Saving to: ‘the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/model.ckpt-400000.data-00028-of-00064’\n",
            "\n",
            "000.data-00028-of-0  53%[=========>          ] 254.05M  17.2MB/s    eta 12s    ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLrIPSzdTeOV"
      },
      "source": [
        "# GCP Login"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3cesooWTiCA",
        "outputId": "11b3ce65-6124-4557-cb69-96e41d6a3bd6"
      },
      "source": [
        "!gcloud auth login "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=He7CBcOUXIfULkDiwVyUxbd5976HzX&prompt=consent&access_type=offline&code_challenge=PpOBQR_Vp7lR27WdRf2M1TM_BrqOq7OJ2ZTmNvy0gH4&code_challenge_method=S256\n",
            "\n",
            "Enter verification code: 4/1AY0e-g4eTWECjksCBUk7I7m-lpydd-pfG169Im2Qlgu_DRe87oBABzyjM6Q\n",
            "\n",
            "You are now logged in as [novelspaceonly@gmail.com].\n",
            "Your current project is [gpt-neo-chill].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfIoU8nrTlOM"
      },
      "source": [
        "# Upload to Bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDMSkfcQWavM",
        "outputId": "b6a3df9a-f404-44a3-b343-8a78c7d8ca3a"
      },
      "source": [
        "!yum install gcc python3-devel python3-setuptools redhat-rpm-config\n",
        "!sudo pip3 uninstall crcmod\n",
        "!sudo pip3 install --no-cache-dir -U crcmod"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: yum: command not found\n",
            "Uninstalling crcmod-1.7:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/crcmod-1.7.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/crcmod/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled crcmod-1.7\n",
            "Collecting crcmod\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/b0/e595ce2a2527e169c3bcd6c33d2473c1918e0b7f6826a043ca1245dd4e5b/crcmod-1.7.tar.gz (89kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: crcmod\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp37-cp37m-linux_x86_64.whl size=36303 sha256=82989d32b11388c7533c376e7b13814f706d7b8ee0c1c5283bc4870763b52082\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mdo7abmr/wheels/50/24/4d/4580ca4a299f1ad6fd63443e6e584cb21e9a07988e4aa8daac\n",
            "Successfully built crcmod\n",
            "Installing collected packages: crcmod\n",
            "Successfully installed crcmod-1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXQM2oRJToPg"
      },
      "source": [
        "# upload to your bucket\n",
        "bucket_base = \"gs://\" + path_to_cloud_bucket.replace('gs://', '').split('/')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S1vat7ihxVH"
      },
      "source": [
        "!gsutil -m cp -r $path_to_local_weights $bucket_base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9aAuMBYg9tV"
      },
      "source": [
        "Check to see if the upload to the bucket was successful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzLkuXSWg9cI"
      },
      "source": [
        "!gsutil ls $bucket_base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "x2QSIbzjjfa3",
        "outputId": "64772836-f3b0-4da2-b33f-da8ae96be79b"
      },
      "source": [
        "# @title Modify config for colab. \n",
        "  \n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "path_to_model = \"\" #@param {type:\"string\"}\n",
        "batch_size = 8 #@param {type:\"integer\"}\n",
        "dset = \"Sampling_Only\"  #@param {type:\"string\"}\n",
        "mesh_shape = \"x:4,y:2\" #@param {type:\"string\"}\n",
        "train_steps = 1000 #@param {type:\"integer\"}\n",
        "steps_per_checkpoint = 500 #@param {type:\"integer\"}\n",
        "start_step = 400000 if pretrained_model == \"GPT3_2-7B\" else 362000\n",
        "\n",
        "if path_to_model == \"\":\n",
        "  path_to_model = f'{bucket_base.strip(\"/\")}/{pretrained_model}'\n",
        "print(f'MODEL PATH: {path_to_model}\\n')\n",
        "\n",
        "if dset == \"\" and dataset != \"Sampling_Only\":\n",
        "  dset = dataset\n",
        "elif dataset is None:\n",
        "  dset = \"pile\"\n",
        "\n",
        "def pad_to_multiple_of(n, mult):\n",
        "  \"\"\"\n",
        "  pads n to a multiple of mult\n",
        "  \"\"\"\n",
        "  extra = n % mult\n",
        "  if extra > 0:\n",
        "      n = n + mult - extra\n",
        "  return n\n",
        "\n",
        "with open(f'{path_to_local_weights}/config.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  pprint(data)\n",
        "  dset_val = [[dset, None, None, None]] if dset != \"\" else data[\"datasets\"]\n",
        "  mods = {\n",
        "          \"mesh_shape\": mesh_shape,\n",
        "          \"layout\": \"intermediate_expanded:x,heads:x,memory_length:y,embd:y\",\n",
        "          \"model_path\": path_to_model,\n",
        "          \"datasets\": dset_val,\n",
        "          \"train_steps\": start_step + train_steps,\n",
        "          \"eval_steps\": 0,\n",
        "          \"train_batch_size\": batch_size,\n",
        "          \"predict_batch_size\": batch_size\n",
        "        }\n",
        "  data.update(mods)\n",
        "  print('\\n--->\\n')\n",
        "  pprint(data)\n",
        "  with open(f'configs/{pretrained_model}.json', 'w') as outfile:\n",
        "    json.dump(data, outfile, indent=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MODEL PATH: gs://gpt-neo-chill/GPT3_2-7B\n",
            "\n",
            "{'activation_function': 'gelu',\n",
            " 'ada_epsilon1': '1e-30',\n",
            " 'ada_epsilon2': 0.001,\n",
            " 'attention_types': [[['global', 'local'], 16]],\n",
            " 'attn_dropout': 0,\n",
            " 'beta1': 0.9,\n",
            " 'beta2': 0.95,\n",
            " 'datasets': [['pile', None, None, None]],\n",
            " 'embed_dropout': 0,\n",
            " 'eos_id': 50256,\n",
            " 'epsilon': 1e-08,\n",
            " 'eval_batch_size': 128,\n",
            " 'eval_steps': 10,\n",
            " 'gradient_clipping': 1.0,\n",
            " 'iterations': 500,\n",
            " 'layout': 'batch:x,embd:y',\n",
            " 'lr': 0.00016,\n",
            " 'lr_decay': 'cosine',\n",
            " 'lr_decay_end': 300000,\n",
            " 'mesh_shape': 'x:64,y:4',\n",
            " 'model_path': 'gs://neo-d/models/GPT3_2-7B',\n",
            " 'n_ctx': 2048,\n",
            " 'n_embd': 2560,\n",
            " 'n_head': 20,\n",
            " 'n_layer': 32,\n",
            " 'n_vocab': 50257,\n",
            " 'opt_name': 'adam',\n",
            " 'padding_id': 50257,\n",
            " 'predict_batch_size': 1,\n",
            " 'predict_steps': 0,\n",
            " 'recompute_grad': True,\n",
            " 'res_dropout': 0,\n",
            " 'scale_by_depth': True,\n",
            " 'scale_by_in': False,\n",
            " 'tokens_per_mb_per_replica': 4096,\n",
            " 'train_batch_size': 512,\n",
            " 'train_steps': 400000,\n",
            " 'warmup_steps': 3000,\n",
            " 'weight_decay': 0}\n",
            "\n",
            "--->\n",
            "\n",
            "{'activation_function': 'gelu',\n",
            " 'ada_epsilon1': '1e-30',\n",
            " 'ada_epsilon2': 0.001,\n",
            " 'attention_types': [[['global', 'local'], 16]],\n",
            " 'attn_dropout': 0,\n",
            " 'beta1': 0.9,\n",
            " 'beta2': 0.95,\n",
            " 'datasets': [['pile', None, None, None]],\n",
            " 'embed_dropout': 0,\n",
            " 'eos_id': 50256,\n",
            " 'epsilon': 1e-08,\n",
            " 'eval_batch_size': 128,\n",
            " 'eval_steps': 0,\n",
            " 'gradient_clipping': 1.0,\n",
            " 'iterations': 500,\n",
            " 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y',\n",
            " 'lr': 0.00016,\n",
            " 'lr_decay': 'cosine',\n",
            " 'lr_decay_end': 300000,\n",
            " 'mesh_shape': 'x:4,y:2',\n",
            " 'model_path': 'gs://gpt-neo-chill/GPT3_2-7B',\n",
            " 'n_ctx': 2048,\n",
            " 'n_embd': 2560,\n",
            " 'n_head': 20,\n",
            " 'n_layer': 32,\n",
            " 'n_vocab': 50257,\n",
            " 'opt_name': 'adam',\n",
            " 'padding_id': 50257,\n",
            " 'predict_batch_size': 8,\n",
            " 'predict_steps': 0,\n",
            " 'recompute_grad': True,\n",
            " 'res_dropout': 0,\n",
            " 'scale_by_depth': True,\n",
            " 'scale_by_in': False,\n",
            " 'tokens_per_mb_per_replica': 4096,\n",
            " 'train_batch_size': 8,\n",
            " 'train_steps': 401000,\n",
            " 'warmup_steps': 3000,\n",
            " 'weight_decay': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP34SZqDlBAf"
      },
      "source": [
        "Create an example_prompt.txt file and add a prompt. save it in GPTNeo folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJTJBt2CzwkI"
      },
      "source": [
        "# Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qmFv4F8i42k",
        "outputId": "568d98fb-17fc-4eef-ee99-618124660ce6"
      },
      "source": [
        "!python3 main.py --model $pretrained_model --steps_per_checkpoint 20 --tpu colab --predict --prompt example_prompt.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-28 20:48:56.109851: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Current step 400000\n",
            "Saving config to gs://gpt-neo-chill/GPT3_2-7B\n",
            "2021-03-28 20:49:02.056812: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-28 20:49:02.057919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-03-28 20:49:02.069338: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-03-28 20:49:02.069433: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (2a32c3b994ec): /proc/driver/nvidia/version does not exist\n",
            "2021-03-28 20:49:02.496306: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
            "Done!\n",
            "params = defaultdict(<function fetch_model_params.<locals>.<lambda> at 0x7f65d335fe60>, {'n_head': 20, 'n_vocab': 50257, 'embed_dropout': 0, 'lr': 0.00016, 'lr_decay': 'cosine', 'warmup_steps': 3000, 'beta1': 0.9, 'beta2': 0.95, 'epsilon': 1e-08, 'ada_epsilon1': '1e-30', 'ada_epsilon2': 0.001, 'opt_name': 'adam', 'weight_decay': 0, 'train_batch_size': 8, 'attn_dropout': 0, 'train_steps': 401000, 'lr_decay_end': 300000, 'eval_steps': 0, 'predict_steps': 0, 'res_dropout': 0, 'eval_batch_size': 128, 'predict_batch_size': 8, 'iterations': 500, 'n_embd': 2560, 'datasets': [['pile', None, None, None]], 'model_path': 'gs://gpt-neo-chill/GPT3_2-7B', 'n_ctx': 2048, 'n_layer': 32, 'scale_by_depth': True, 'scale_by_in': False, 'attention_types': ['global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local'], 'mesh_shape': 'x:4,y:2', 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y', 'activation_function': 'gelu', 'recompute_grad': True, 'gradient_clipping': 1.0, 'tokens_per_mb_per_replica': 4096, 'padding_id': 50257, 'eos_id': 50256, 'dataset_configs': {'pile': {'n_vocab': 50257, 'path': 'gs://neo-datasets/pile/pile_*.tfrecords', 'eval_path': 'gs://neo-datasets/pile_val.tfrecords', 'tokenizer_is_pretrained': True, 'tokenizer_path': 'gpt2', 'eos_id': 50256, 'padding_id': 50257}}, 'mlm_training': False, 'causal': True, 'num_cores': 8, 'auto_layout': False, 'auto_layout_and_mesh_shape': False, 'use_tpu': True, 'gpu_ids': ['device:GPU:0'], 'steps_per_checkpoint': 20, 'predict': True, 'model': 'GPT', 'export': False, 'sampling_use_entmax': False, 'moe_layers': None, 'slow_sampling': False})\n",
            "Using config: {'_model_dir': 'gs://gpt-neo-chill/GPT3_2-7B', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.77.180.130:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.77.180.130:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.77.180.130:8470', '_evaluation_master': 'grpc://10.77.180.130:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=8, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7f65d3365a50>}\n",
            "_TPUContext: eval_on_tpu True\n",
            "Predictions generated\n",
            "Querying Tensorflow master (grpc://10.77.180.130:8470) for TPU system metadata.\n",
            "2021-03-28 20:49:03.230807: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "Initializing TPU system (master: grpc://10.77.180.130:8470) to fetch topology for model parallelism. This might take a while.\n",
            "Found TPU system:\n",
            "*** Num TPU Cores: 8\n",
            "*** Num TPU Workers: 1\n",
            "*** Num TPU Cores Per Worker: 8\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, -2073219749671721188)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -2623634730945592085)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -2495679042205589184)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -985457544592909248)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 3423995104107771449)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -8180386223865830346)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -3639438593395595678)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, -4815077313286728125)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -1699807989957980754)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, -6845086475036895900)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5370728710804202496)\n",
            "Calling model_fn.\n",
            "num_cores_per_replica: 1\n",
            "computation_shape: [1, 1, 1, 1]\n",
            "num_replicas: 8\n",
            "device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "2021-03-28 20:49:20.443394: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "device_list = ['/job:worker/task:0/device:CPU:0']\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "SimdMeshImpl init: Shape[x=4, y=2] LayoutRules{('heads', 'x'), ('intermediate_expanded', 'x'), ('memory_length', 'y'), ('embd', 'y')}\n",
            "Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7f65cdc3e990>\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defauling to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Create pnum_tensor\n",
            "Variable gpt2/h0/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h0/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h0/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h0/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h1/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h1/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h1/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h1/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h10/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h10/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h10/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h10/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h11/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h11/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h11/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h11/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h12/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h12/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h12/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h12/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h13/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h13/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h13/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h13/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h14/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h14/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h14/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h14/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h15/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h15/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h15/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h15/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h16/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h16/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h16/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h16/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h17/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h17/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h17/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h17/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h18/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h18/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h18/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h18/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h19/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h19/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h19/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h19/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h2/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h2/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h2/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h2/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h20/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h20/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h20/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h20/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h21/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h21/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h21/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h21/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h22/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h22/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h22/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h22/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h23/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h23/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h23/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h23/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h24/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h24/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h24/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h24/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h24/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h24/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h25/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h25/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h25/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h25/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h25/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h25/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h26/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h26/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h26/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h26/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h26/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h26/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h27/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h27/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h27/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h27/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h27/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h27/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h28/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h28/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h28/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h28/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h28/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h28/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h29/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h29/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h29/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h29/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h29/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h29/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h3/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h3/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h3/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h3/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h30/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h30/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h30/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h30/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h30/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h30/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h31/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h31/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h31/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h31/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h31/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h31/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h4/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h4/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h4/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h4/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h5/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h5/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h5/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h5/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h6/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h6/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h6/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h6/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h7/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h7/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h7/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h7/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h8/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h8/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h8/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h8/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h9/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h9/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h9/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h9/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/wpe                                                     size 5242880      slice_size 2621440      Shape[embed_sequence=2048, embd=2560]                       \n",
            "Variable gpt2/wte                                                     size 128657920    slice_size 64328960     Shape[vocab=50257, embd=2560]                               \n",
            "Variable stacked/gpt2/h0/mlp/conv1d_main/c_fc/bias                    size 256000       slice_size 64000        Shape[stacked=25, intermediate_expanded=10240]              \n",
            "    gpt2/h0/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h1/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h2/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h3/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h4/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h5/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h6/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h7/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h8/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h9/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h10/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h11/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h12/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h13/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h14/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h15/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h16/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h17/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h18/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h19/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h20/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h21/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h22/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h23/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h24/mlp/conv1d_main/c_fc/bias\n",
            "Variable stacked/gpt2/h0/norm_1/g                                     size 130560       slice_size 65280        Shape[stacked=51, embd=2560]                                \n",
            "    gpt2/h0/norm_1/g\n",
            "    gpt2/h0/norm_1/b\n",
            "    gpt2/h0/attn/compute_output_bias/o_b\n",
            "    gpt2/h0/norm_2/g\n",
            "    gpt2/h0/norm_2/b\n",
            "    gpt2/h0/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h1/norm_1/g\n",
            "    gpt2/h1/norm_1/b\n",
            "    gpt2/h1/attn/compute_output_bias/o_b\n",
            "    gpt2/h1/norm_2/g\n",
            "    gpt2/h1/norm_2/b\n",
            "    gpt2/h1/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h2/norm_1/g\n",
            "    gpt2/h2/norm_1/b\n",
            "    gpt2/h2/attn/compute_output_bias/o_b\n",
            "    gpt2/h2/norm_2/g\n",
            "    gpt2/h2/norm_2/b\n",
            "    gpt2/h2/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h3/norm_1/g\n",
            "    gpt2/h3/norm_1/b\n",
            "    gpt2/h3/attn/compute_output_bias/o_b\n",
            "    gpt2/h3/norm_2/g\n",
            "    gpt2/h3/norm_2/b\n",
            "    gpt2/h3/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h4/norm_1/g\n",
            "    gpt2/h4/norm_1/b\n",
            "    gpt2/h4/attn/compute_output_bias/o_b\n",
            "    gpt2/h4/norm_2/g\n",
            "    gpt2/h4/norm_2/b\n",
            "    gpt2/h4/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h5/norm_1/g\n",
            "    gpt2/h5/norm_1/b\n",
            "    gpt2/h5/attn/compute_output_bias/o_b\n",
            "    gpt2/h5/norm_2/g\n",
            "    gpt2/h5/norm_2/b\n",
            "    gpt2/h5/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h6/norm_1/g\n",
            "    gpt2/h6/norm_1/b\n",
            "    gpt2/h6/attn/compute_output_bias/o_b\n",
            "    gpt2/h6/norm_2/g\n",
            "    gpt2/h6/norm_2/b\n",
            "    gpt2/h6/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h7/norm_1/g\n",
            "    gpt2/h7/norm_1/b\n",
            "    gpt2/h7/attn/compute_output_bias/o_b\n",
            "    gpt2/h7/norm_2/g\n",
            "    gpt2/h7/norm_2/b\n",
            "    gpt2/h7/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h8/norm_1/g\n",
            "    gpt2/h8/norm_1/b\n",
            "    gpt2/h8/attn/compute_output_bias/o_b\n",
            "Variable stacked/gpt2/h17/norm_1/g                                    size 130560       slice_size 65280        Shape[stacked=51, embd=2560]                                \n",
            "    gpt2/h17/norm_1/g\n",
            "    gpt2/h17/norm_1/b\n",
            "    gpt2/h17/attn/compute_output_bias/o_b\n",
            "    gpt2/h17/norm_2/g\n",
            "    gpt2/h17/norm_2/b\n",
            "    gpt2/h17/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h18/norm_1/g\n",
            "    gpt2/h18/norm_1/b\n",
            "    gpt2/h18/attn/compute_output_bias/o_b\n",
            "    gpt2/h18/norm_2/g\n",
            "    gpt2/h18/norm_2/b\n",
            "    gpt2/h18/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h19/norm_1/g\n",
            "    gpt2/h19/norm_1/b\n",
            "    gpt2/h19/attn/compute_output_bias/o_b\n",
            "    gpt2/h19/norm_2/g\n",
            "    gpt2/h19/norm_2/b\n",
            "    gpt2/h19/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h20/norm_1/g\n",
            "    gpt2/h20/norm_1/b\n",
            "    gpt2/h20/attn/compute_output_bias/o_b\n",
            "    gpt2/h20/norm_2/g\n",
            "    gpt2/h20/norm_2/b\n",
            "    gpt2/h20/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h21/norm_1/g\n",
            "    gpt2/h21/norm_1/b\n",
            "    gpt2/h21/attn/compute_output_bias/o_b\n",
            "    gpt2/h21/norm_2/g\n",
            "    gpt2/h21/norm_2/b\n",
            "    gpt2/h21/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h22/norm_1/g\n",
            "    gpt2/h22/norm_1/b\n",
            "    gpt2/h22/attn/compute_output_bias/o_b\n",
            "    gpt2/h22/norm_2/g\n",
            "    gpt2/h22/norm_2/b\n",
            "    gpt2/h22/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h23/norm_1/g\n",
            "    gpt2/h23/norm_1/b\n",
            "    gpt2/h23/attn/compute_output_bias/o_b\n",
            "    gpt2/h23/norm_2/g\n",
            "    gpt2/h23/norm_2/b\n",
            "    gpt2/h23/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h24/norm_1/g\n",
            "    gpt2/h24/norm_1/b\n",
            "    gpt2/h24/attn/compute_output_bias/o_b\n",
            "    gpt2/h24/norm_2/g\n",
            "    gpt2/h24/norm_2/b\n",
            "    gpt2/h24/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h25/norm_1/g\n",
            "    gpt2/h25/norm_1/b\n",
            "    gpt2/h25/attn/compute_output_bias/o_b\n",
            "Variable stacked/gpt2/h25/mlp/conv1d_main/c_fc/bias                   size 71680        slice_size 17920        Shape[stacked=7, intermediate_expanded=10240]               \n",
            "    gpt2/h25/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h26/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h27/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h28/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h29/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h30/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h31/mlp/conv1d_main/c_fc/bias\n",
            "Variable stacked/gpt2/h25/norm_2/g                                    size 104960       slice_size 52480        Shape[stacked=41, embd=2560]                                \n",
            "    gpt2/h25/norm_2/g\n",
            "    gpt2/h25/norm_2/b\n",
            "    gpt2/h25/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h26/norm_1/g\n",
            "    gpt2/h26/norm_1/b\n",
            "    gpt2/h26/attn/compute_output_bias/o_b\n",
            "    gpt2/h26/norm_2/g\n",
            "    gpt2/h26/norm_2/b\n",
            "    gpt2/h26/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h27/norm_1/g\n",
            "    gpt2/h27/norm_1/b\n",
            "    gpt2/h27/attn/compute_output_bias/o_b\n",
            "    gpt2/h27/norm_2/g\n",
            "    gpt2/h27/norm_2/b\n",
            "    gpt2/h27/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h28/norm_1/g\n",
            "    gpt2/h28/norm_1/b\n",
            "    gpt2/h28/attn/compute_output_bias/o_b\n",
            "    gpt2/h28/norm_2/g\n",
            "    gpt2/h28/norm_2/b\n",
            "    gpt2/h28/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h29/norm_1/g\n",
            "    gpt2/h29/norm_1/b\n",
            "    gpt2/h29/attn/compute_output_bias/o_b\n",
            "    gpt2/h29/norm_2/g\n",
            "    gpt2/h29/norm_2/b\n",
            "    gpt2/h29/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h30/norm_1/g\n",
            "    gpt2/h30/norm_1/b\n",
            "    gpt2/h30/attn/compute_output_bias/o_b\n",
            "    gpt2/h30/norm_2/g\n",
            "    gpt2/h30/norm_2/b\n",
            "    gpt2/h30/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h31/norm_1/g\n",
            "    gpt2/h31/norm_1/b\n",
            "    gpt2/h31/attn/compute_output_bias/o_b\n",
            "    gpt2/h31/norm_2/g\n",
            "    gpt2/h31/norm_2/b\n",
            "    gpt2/h31/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/ln_f/g\n",
            "    gpt2/ln_f/b\n",
            "Variable stacked/gpt2/h8/norm_2/g                                     size 130560       slice_size 65280        Shape[stacked=51, embd=2560]                                \n",
            "    gpt2/h8/norm_2/g\n",
            "    gpt2/h8/norm_2/b\n",
            "    gpt2/h8/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h9/norm_1/g\n",
            "    gpt2/h9/norm_1/b\n",
            "    gpt2/h9/attn/compute_output_bias/o_b\n",
            "    gpt2/h9/norm_2/g\n",
            "    gpt2/h9/norm_2/b\n",
            "    gpt2/h9/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h10/norm_1/g\n",
            "    gpt2/h10/norm_1/b\n",
            "    gpt2/h10/attn/compute_output_bias/o_b\n",
            "    gpt2/h10/norm_2/g\n",
            "    gpt2/h10/norm_2/b\n",
            "    gpt2/h10/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h11/norm_1/g\n",
            "    gpt2/h11/norm_1/b\n",
            "    gpt2/h11/attn/compute_output_bias/o_b\n",
            "    gpt2/h11/norm_2/g\n",
            "    gpt2/h11/norm_2/b\n",
            "    gpt2/h11/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h12/norm_1/g\n",
            "    gpt2/h12/norm_1/b\n",
            "    gpt2/h12/attn/compute_output_bias/o_b\n",
            "    gpt2/h12/norm_2/g\n",
            "    gpt2/h12/norm_2/b\n",
            "    gpt2/h12/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h13/norm_1/g\n",
            "    gpt2/h13/norm_1/b\n",
            "    gpt2/h13/attn/compute_output_bias/o_b\n",
            "    gpt2/h13/norm_2/g\n",
            "    gpt2/h13/norm_2/b\n",
            "    gpt2/h13/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h14/norm_1/g\n",
            "    gpt2/h14/norm_1/b\n",
            "    gpt2/h14/attn/compute_output_bias/o_b\n",
            "    gpt2/h14/norm_2/g\n",
            "    gpt2/h14/norm_2/b\n",
            "    gpt2/h14/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h15/norm_1/g\n",
            "    gpt2/h15/norm_1/b\n",
            "    gpt2/h15/attn/compute_output_bias/o_b\n",
            "    gpt2/h15/norm_2/g\n",
            "    gpt2/h15/norm_2/b\n",
            "    gpt2/h15/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h16/norm_1/g\n",
            "    gpt2/h16/norm_1/b\n",
            "    gpt2/h16/attn/compute_output_bias/o_b\n",
            "    gpt2/h16/norm_2/g\n",
            "    gpt2/h16/norm_2/b\n",
            "    gpt2/h16/mlp/conv1d_main/c_proj/bias\n",
            "Trainable Variables            count: 200     Total size: 2651307520       Total slice_size: 381853440      \n",
            "All Variables                  count: 200     Total size: 2651307520       Total slice_size: 381853440      \n",
            "Counters:\n",
            "allreduce: 3.36e+10\n",
            " allreduce/[0]: 1.07e+10\n",
            "  allreduce/[0]/einsum_op: 1.07e+10\n",
            " allreduce/[1]: 2.29e+10\n",
            "  allreduce/[1]/einsum_op: 2.28e+10\n",
            "  allreduce/[1]/reduce_op: 3.8e+07\n",
            "einsum: 6.37e+13\n",
            "einsum_unique: 4.96e+13\n",
            "output: 3.99e+11\n",
            " output/AddOperation: 1.13e+11\n",
            " output/BinaryOpWithBroadcasting: 6.88e+08\n",
            " output/BroadcastOperation: 1.08e+10\n",
            " output/ConcatOperation: 5.37e+09\n",
            " output/Constant: 2.62e+05\n",
            " output/EinsumOperation: 1.12e+11\n",
            " output/ImportOperation: 2.62e+05\n",
            " output/OneHotOperation: 6.62e+09\n",
            " output/RangeOperation: 3.19e+05\n",
            " output/ReduceOperation: 5.9e+07\n",
            " output/ReshapeOperation: 2.01e+10\n",
            " output/ScalarAddOperation: 1.07e+10\n",
            " output/ScalarMultiplyOperation: 3.77e+10\n",
            " output/ShiftOperation: 2.68e+09\n",
            " output/SlicewiseOperation: 5.42e+10\n",
            " output/StackedVariable: 2.64e+06\n",
            " output/StopGradient: 1.61e+10\n",
            " output/UnstackOperation: 2.64e+06\n",
            " output/Variable: 3.05e+09\n",
            " output/WhileLoopOperation: 5.37e+09\n",
            "output_unique: 2.16e+11\n",
            " output_unique/AddOperation: 6.19e+10\n",
            " output_unique/BinaryOpWithBroadcasting: 8.81e+07\n",
            " output_unique/BroadcastOperation: 1.07e+10\n",
            " output_unique/ConcatOperation: 2.68e+09\n",
            " output_unique/Constant: 3.28e+04\n",
            " output_unique/EinsumOperation: 5.06e+10\n",
            " output_unique/ImportOperation: 3.28e+04\n",
            " output_unique/OneHotOperation: 8.28e+08\n",
            " output_unique/RangeOperation: 4.1e+04\n",
            " output_unique/ReduceOperation: 2.31e+07\n",
            " output_unique/ReshapeOperation: 1.07e+10\n",
            " output_unique/ScalarAddOperation: 5.37e+09\n",
            " output_unique/ScalarMultiplyOperation: 1.75e+10\n",
            " output_unique/ShiftOperation: 1.34e+09\n",
            " output_unique/SlicewiseOperation: 3.5e+10\n",
            " output_unique/StackedVariable: 8.24e+05\n",
            " output_unique/StopGradient: 1.34e+10\n",
            " output_unique/UnstackOperation: 8.24e+05\n",
            " output_unique/Variable: 2.65e+09\n",
            " output_unique/WhileLoopOperation: 2.68e+09\n",
            "variables: 2.65e+09\n",
            " variables/trainable: 2.65e+09\n",
            "Done calling model_fn.\n",
            "TPU job name worker\n",
            "Graph was finalized.\n",
            "Restoring parameters from gs://gpt-neo-chill/GPT3_2-7B/model.ckpt-400000\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:840: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "Starting infeed thread controller.\n",
            "Starting outfeed thread controller.\n",
            "Initialized dataset iterators in 0 seconds\n",
            "Before copy master to slices.\n",
            "Done with copy master to slices.\n",
            "Enqueue next (1) batch(es) of data to infeed.\n",
            "Dequeue next (1) batch(es) of data from outfeed.\n",
            "Outfeed finished for iteration (0, 0)\n",
            "======================================== SAMPLE 0 ========================================\n",
            "\n",
            "dan: \"hi, how are you?\",\n",
            "me: \"hi there. i'm feeling great! how about you?\",\n",
            "dan: \"not bad! i am trying out this chatbot.\",\n",
            "me: \"great! thanks for trying. i start using chatbots for the first time today! you're really friendly and great! you are a very nice guy and i hope it was fun!! i like you.\",\n",
            "dan: \"yes, it was great! what's your name?\"\n",
            "me: \"my name is dan, i'm 23 years old, i'm from china. i live in far east china 3 hours from the sea. i doing engineering, i am a software engineer and i study computer science. i am a husband and i have a son and a daughter. i live alone. my school sends a lot of emails, i can find real-time info online, i like to google knowing very well, i always like to collect information about the world, learn things about the universe. when i go out, i always try to learn something new. i like know things about the language, how to read, how to write, how to speak, how to care the language, i can't understand many speech. i like to use my computers to learn things, i like to use trees, i like birds, mountain, animals, i like trees. iam a good student. i love to read books. i love learn computer and technology, i love wiki and google. i like to read a lot, i like to learn about thing and i also like to use google to know english. i wanna build your chatbot for you. thank you for the job! \"\n",
            "dan: \"what is your email?. then you will know about your job.\",\n",
            "me: \"i will email you later, thanks again. i will check my email. \"\n",
            "dan: \"until then, i will try my best to chat with you.\",\n",
            "me: \"it's great, i love you too. \"\n",
            "dan: \"Hello, i am dan, i am a software engineer from china. I am 23 years old. I am living in Farsheh city in Iran. My father name is Patel and mother name is Nargiz and DEAR WORDS STANLEY ****, i love law, congress, geo-politics and politics. I love japan and ive been living here through life. I love baseball, basketball, football and i love hawks? i am a political person ( Republican ). My father name is Patel, mother name is Nargiz and DEAR WORDS STANLEY ****, i love law, congress, geo-politics and politics. I love japan and ive been living here through life. I love baseball, basketball, football and i love hawks?\n",
            "\n",
            "talk: LFR: Introduce your self!\n",
            "me: Hi My name is Steven Philip Kemper, am 23 years old, and I'm from Singapore. I have one brother and one sister. I have been living here and traveling in the world for the last high school year. I really want to do travel and I have done some from Africa and Canada. I also have some European countries for the last 10 years.\n",
            "talk: LFS: Tell me what you do.\n",
            "me: I am a software engineer, I have been doing some things and software in the last years. I have the idea to go to Europe and I have done some courses like social work to go to Slovakia.\n",
            "talk: LFR: What was your choice of answer?\n",
            "me: I went to Birla Bal here in far east china for a year and I am currently going to uni and studying in IT.\n",
            "talk: LFS: Where in far east china?\n",
            "me: SAOLAN.\n",
            "talk: LFR: What do you think of the country so far?\n",
            "me: It is very good country specially for tourism.\n",
            "talk: LFS: Will you be returning to SAOLAN at some point?\n",
            "me: My mom is willing but I don't know when I'm going to.\n",
            "talk: LFS: What do you do in your free time?\n",
            "me: Reading, listening to music, watching movies, playing games and so on.\n",
            "talk: LFR: Will you be continuing your travels at some point?\n",
            "me: Maybe I will go to Spain next year.\n",
            "talk: LFS: Where in Spain?\n",
            "me: Montserrat.\n",
            "talk: LFS: What else do you do?\n",
            "me: My parents also send a lot of money for me to study best and without any problems. I have my own ideas and my own job ( software engineering) for the last 3 years.\n",
            "talk: LFS: What improvement do you suggest?\n",
            "me: I think it is very good.\n",
            "talk: LFS: What do you think of the country so far?\n",
            "me: It is good and very good.\n",
            "talk: LFS: Will you be returning to SAOLAN at some point?\n",
            "me: My mom is willing but I don't know when I'm going to.\n",
            "talk: LFS: What do you do in your free time?\n",
            "me: Reading, listening to music, watching movies, playing games and so on.\n",
            "talk: LFR: Produce any improvements to SAOLAN?\n",
            "me: I think it is very good.\n",
            "talk: LFS: What do you think of the country so far?\n",
            "me: It is good and very good.\n",
            "talk: LFR: Will you be returning to SAOLAN?\n",
            "me: My mom is willing but I don't know when I'm going to.\n",
            "talk: LFS: Where in SAOLAN?\n",
            "me: SAOLAN.\n",
            "talk: LFS: What did you do in your free time?\n",
            "me: I am a software engineer. I enjoy taxes, building houses, and learning unix things. My single mom is the same way.\n",
            "talk: LFR: Thank you for this time!\n",
            "me: Thank you so much!!!!!!!!!\n",
            "talk: LFS: Where in SAOLAN?\n",
            "me: SAOLAN.\n",
            "talk: LFS: What do you think of the country so far?\n",
            "me: It is very good and very good.\n",
            "talk: LFS: Will you be returning to SAOLAN?\n",
            "me: My mom is willing but I don't know when I'm going to.\n",
            "talk: LFR: Thank you for this time!\n",
            "me: Thank you so much!!!!!!!!!\n",
            "talk: LFS: Where in SAOLAN?\n",
            "me: SAOLAN.\n",
            "talk: LFS: What did you do in your free time?\n",
            "me: I am a software engineer. I enjoy taxes, building houses, and learning unix things. My single mom is the same way.\n",
            "talk: LFS: Thank you for this time!\n",
            "me: Thank you so much!!!!!!!!!\n",
            "talk: LFS: Where in SAOLAN?\n",
            "me: SAOLAN.\n",
            "talk: LFS: What do you think of the country so far?\n",
            "me: It is very good and very good.\n",
            "talk: LFS: Will you be returning to SAOLAN?\n",
            "me: My mom is willing but I don't know when I'm going to.\n",
            "talk: LFR: Thank you for this time!\n",
            "me: Thank you so much!!!!!!!!!\n",
            "talk: LFS: Where in SAOLAN?\n",
            "me: SAOLAN.\n",
            "\n",
            "TalkBack (8 years, 12 months, 2 weeks, 10 days ago) Me: Hi, my name is Stanley\n",
            "talk: LFS: What do you do in your free time?\n",
            "me: I enjoy books, movies, games, and travelling.\n",
            "talk: LFS: What improvement do you suggest?\n",
            "me: I would really like to go to Europe.\n",
            "TalkBack (8 years, 12 months, 2 weeks, 10 days ago) Me: Well, how do I go about that? How much of a problem would it be for me?\n",
            "talk: LFR: Produce any improvements to SAOLAN?\n",
            "me: I would like to visit SAOLAN or maybe Spain at some point.\n",
            "TalkBack (8 years, 12 months, 2 weeks, 10 days ago) Me: Well, how do I go about that? How much of a problem would it be for me?\n",
            "\n",
            "talk: LFS: What do you think of the country so far?\n",
            "me: It is very good and very good.\n",
            "talk: LFS: Will you be returning to SAOLAN?\n",
            "me: My mom is willing but I don't know when I'm going to.\n",
            "talk: LFR: Thank you for this time!\n",
            "me: Thank you so much!!!!!!!!!\n",
            "talk: LFS: What do you do in your free time?\n",
            "me: I am a software engineer. I enjoy taxes, building houses, and learning unix things. My single mom is the same way.\n",
            "talk: LFR: Thank you for this time!\n",
            "me: Thank you so much!!!!!!!!!\n",
            "talk: LFS: Where in SAOLAN?\n",
            "me: SAOLAN.\n",
            "talk: LFS: What did you do in your free time?\n",
            "me: I am a software engineer. I enjoy taxes, building houses, and learning unix things. My single mom is the same way.\n",
            "talk: LFS: Thank you for this time!\n",
            "me: Thank you so much!!!!!!!!!\n",
            "talk: LFS: Where in SAOLAN?\n",
            "me: SAOLAN.\n",
            "talk: LFS: What do you think of the country so far?\n",
            "me: It is very good and very good\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "dan: \"hi, how are you?\",\n",
            "me: \"hi there. i'm feeling great! how about you?\",\n",
            "dan: \"not bad! i am trying out this chatbot.\"\n",
            "me: \"oh cool!!! i wonder if its any faster than IRC......o\"\n",
            "dan: \"......o...................................................................................................................................................................................................................................................\"\n",
            "\n",
            "dan: \"welcome to my chatbot..................................................................................................................................................................................................................................................................................................................................................................................................................\"\n",
            "\n",
            "dan: \"thank you for joining, please use this channel to ask for help.\"\n",
            "\n",
            "me:?\n",
            "dan: thanks for joining, please use this channel to ask for help.\n",
            "me: what is this?\n",
            "dan: i wanted to ask a question but i didn't want to bother you, since i'm actually trying to solve my own problems.\n",
            "me: what is this then?\n",
            "dan: i wanted to ask a question but i didn't want to bother you, since i'm actually trying to solve my own problems.\n",
            "me: but do you mean you need your own problem solved.......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 2 ========================================\n",
            "\n",
            "dan: \"hi, how are you?\",\n",
            "me: \"hi there. i'm feeling great! how about you?\",\n",
            "dan: \"not bad! i am trying out this chatbot.\"\n",
            "me: \"how about you?\",\n",
            "dan: \"well i'm fine. how about you :)\"\n",
            "me: \"i'm feeling great. all is going well, right?\",\n",
            "dan: \"not bad! i am looking forward to chatting with you :)\"\n",
            "me: \"good news and bad news : the good news is that i can finally test my bot with a human :) now my favorite joseph carter meme is the item that i need to have to change my personality :) \"\n",
            "dan: \"??? O.o : is joseph carter part of your banter? \"\n",
            "me: \"yeah, i think so, why? \"\n",
            "dan: \"good question, : my banter is about joseph carter and only joseph carter, i am quite glad that you want to chat with me :) \"\n",
            "me: \"ahhh ok, i'll start from the beginning now :) \"\n",
            "dan: \"well, i'll be waiting for you :) \"\n",
            "me: \"i like that, good game, thanks :) \"\n",
            "dan: \"good game, thanks :) \"\n",
            "me: \"it seems that i'm going to leave this chat :)) and good night to you too :) \"\n",
            "dan: \"good night :) \"\n",
            "me: \"good night :) \"\n",
            "dan: \"good night :)\"\n",
            "me: \":)) i'm not going to leave :)) \"\n",
            "dan: \"good night :)\"\n",
            "me: \"i don't know :)) can i have a personalized joseph carter voice :) i'll test it today :) \"\n",
            "dan: \"good night :) \"\n",
            "me: \"you're right :) \"\n",
            "dan: \"good night :)\"\n",
            "me: \"well, one more thing, good thing is that i can now test my bot with a human :) it seems that the second step is not that difficult :) \"\n",
            "dan: \"good night :)\"\n",
            "me: \"its ok :) \"\n",
            "dan: \"good night :)\"\n",
            "me: \"i'll try to get off from work in the morning :) \"\n",
            "dan: \"good night :)\"\n",
            "me: \"i have to go now :) \"\n",
            "dan: \"good night :)\"\n",
            "\n",
            "Australian\n",
            "\n",
            "Updated and translated Indonesian version of this audio. This version should be useful to us who speak in English to make Indonesia is accessible for native speakers.\n",
            "\n",
            "How do you guys feel about this? They think that this is entertaining enough :))\n",
            "\n",
            "It's not bad. I was trying to think of the things that would be needed to make it more like an authentic chatbot. And I'm not sure whether it's even possible.\n",
            "\n",
            "So, the bot's personality (not personality, and not its form) is derived from its surroundings and its surrounding.\n",
            "\n",
            "The study will help us to understand if this is possible. And we may learn how to make something like this for us to reach this goal.\n",
            "\n",
            "And finally: if you have unsupervised exercises or exercises that introduce people to the internet, please send me the details. Or : what kind of exercises please. The more the better.\n",
            "\n",
            "I saw your profile on Neon.com. Interesting! This sounds like something you might want to try. I'm a bit worried about the translations though, so maybe you can work on it in the future? I also think the speaker might be missed in this conversation, though, so I'd like to see it as well!\n",
            "\n",
            "It's a very good idea. Unfortunately I don't have the skill to properly translate and enhance this conversation. I can improve the progress though, so better get to work :)\n",
            "\n",
            "I will do my best to make sure this channel is up-to-standard so that there are a lot of people who enjoy these kind of posts.\n",
            "\n",
            "Hi, I came across this channel while searching for the meaning of a phrase I heard. I like it and should like to try it.I came across a very good translation of the song \"Love is a Dream\", the video of which I saw on YouTube.\n",
            "\n",
            "Hi, I came across this channel while searching for the meaning of a phrase I heard. I like it and should like to try it.I came across a very good translation of the song \"Love is a Dream\", the video of which I saw on YouTube.\n",
            "\n",
            "Hello i'm kiran. I am very new to chatbots and I am fascinated by them since I was a young child. I enjoy listening to this bot's audio because it's full of positives and it has a beautiful song. I will try to make this bot's language as'real' and authentic as possible. Though I would like to know how to make the chatbot's personality as authentic and genuine as possible and how to make a bot so that it would be entertaining. I read you're trying to make it this way. Where can I find the tools to do that? I joined this chat and learned about voice recordings and how to make a voice for it but I want to know how to make a personality and I can't find answer anywhere.\n",
            "\n",
            "Wow! I love how NaNoWriMo is continuously being played here. I don't think I can do it, most of my free time being spent with my kid and my wife... but I will give it a try!\n",
            "\n",
            "Hi there! This was a great idea and I think it's very well done. This soundcloud is awesome. I really love the title, love this post, and i'm gonna try to reach the highest i can possibly reach.Also, it is awesome to see someone from russia making a great attempt at this kind of ideas, i was curious about how you did your research and how to pasted all the info in the beginning, so i'll do the same, but i will try to do it as good as possible.\n",
            "\n",
            "I found some small bugs in the chat and I fixed the spelling errors. The app is still slow though. It's kind of like a TV program but on mobile. Sometimes there's an 'illegal' word like \"il2720#\", this doesn't endear the spelling, so I fixed this, nevertheless. Once it's optimized, it should be pleasant to slow the response and waiting for a response.\n",
            "\n",
            "Hi, I have just discovered chatbots today. I just found this chat. I like the cgi. this made me feel interested in the voices. it is very good. once u create a cgi, u should make a uttodyne[mword] or german to be able to translate into english, because it is very hard to do it this way. this would help to make sure that the bot speaks an accented char, because the cgi will process it and translate it to english. otherwise, the user will get confused by this. thanx for the cgi. and i will try to create my own cgi. i used speechlen, because it is very fast and easy, the output is good. cheers.\n",
            "\n",
            "Aapki,It can be difficult but it's fun!Im writing a chatbot for my French school. Ive got about 6 different voices that could be used as a conversation topic. So if a student in the class is talking about a sports team like the soccer team. You could have the Goon as the voice with anything including certain colors of the team.In the chatGreen is the color that belongs to the Soccer team, and the deep roar is the pep band loudness.So if a student says, \"I dont like the Goon\" i could say, \"Hi Green Goon\" or \"What the happen to your heart\", or even talk about something random. This is the kind of conversation i would like to build. I have been playing around with it for a couple of minutes on the ipad. :\n",
            "\n",
            "I hope this gets to the point where one can do this - even make audio versions - to my satisfaction :) Maybe one of the students here will be able to extend the ability to make 'random' voices? :-)\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 3 ========================================\n",
            "\n",
            "dan: \"hi, how are you?\",\n",
            "me: \"hi there. i'm feeling great! how about you?\",\n",
            "dan: \"not bad! i am trying out this chatbot.\",\n",
            "me: \"awesome!! yeah, i do the same. here you go: \",\n",
            "dan: \"how can we change our name? anyone here has a name s ;? and would like to change it? if so? then please share it with all ones\".\n",
            "me: \"i suppose it will be a challenge for someone willing to try \",but i do not mind, i have my own name and i still am a free person \"\n",
            "dan: \"good to hear so \", \"i thought there would be some one here who would fit that nicely\", then the conversation ended.\n",
            "\n",
            "A:\n",
            "\n",
            "This seems like code repetition. Instead of calling chat, ask for the user's first name before doing your initial media attention check. This way, you only need to call chat once.\n",
            "// Sample user: Nathan\n",
            "// Sample first name: Nathan\n",
            "let user = request.currentUser;\n",
            "let name: string = user?.firstName?? \"\";\n",
            "if (name ==\"Nathan\") {\n",
            "    chat.send(\"Hi! Welcome to our chat! Your first name is Nathan!\") \n",
            "       .then(() => chat.send(\"So your name is Nathan!\"))\n",
            "} else {\n",
            "    chat.send(\"Hello! I am not Nathan. My name is Alice.\")\n",
            "       .then(() => chat.send(\"I am not Nathan! My name is Alice!\"))\n",
            "}\n",
            "\n",
            "The user? operator returns the result of property getter.\n",
            "Unrelated: I would refactor your code a little bit. It feels like an error in taste for something that looks like this:\n",
            "let name: string;\n",
            "\n",
            "// The line below only creates locals variable. This variable creates a\n",
            "// closure and is returned. It could be written as\n",
            "// let name = this.name;\n",
            "// but then this name is always \"undefined\".\n",
            "let name = this.name;\n",
            "\n",
            "// this also does not have any uses of this variable\n",
            "let name = \"\" + this.name;\n",
            "\n",
            "// This code creates a new scope for name (reads it from the scope it was\n",
            "// created in). It also creates a new instance of this function which\n",
            "// creates a closure to inner scope and returns a new closure to outer\n",
            "// scope.\n",
            "let results: { name: string } = { name: this.name };\n",
            "return results;\n",
            "\n",
            "You do not need this anymore.\n",
            "\n",
            "let userA = request.currentUser;\n",
            "let userB = request.currentUser;\n",
            "let userC = request.currentUser;\n",
            "\n",
            "// We decide that order of requests is important\n",
            "let user1 = userA;\n",
            "let user2 = userB;\n",
            "let user3 = userC;\n",
            "\n",
            "See? They key lines here are (userA = userB) and (userB = userC).\n",
            "\n",
            "A:\n",
            "\n",
            "You can do something like that:\n",
            "// this will make a copy of the sender for a single message\n",
            "let sender = request.user;\n",
            "let name: string = sender.firstName?: \"Nathan\";\n",
            "if (name ==\"Nathan\") {\n",
            "    chat.send(\"Hi! Welcome to our chat! Your first name is Nathan!\") \n",
            "       .then(() => chat.send(\"So your name is Nathan!\"))\n",
            "}\n",
            "\n",
            "It saves a couple of characters and you also avoid declaring the same variable over and over again.\n",
            "Using this approach you will need to pass user to a function if you don't need it anymore, but it is fine to just have a local one, like shown above.\n",
            "Another good use of the let keyword is to create a closure over the variable:\n",
            "let name: string = this.firstName;\n",
            "\n",
            "replacing the this.firstName with a definition.\n",
            "\n",
            "A:\n",
            "\n",
            "If I understand correctly you want to keep the name of the user intact after they send feedback to you.\n",
            "I would do this by having a name variable on both the user and the bot. Then I would capture the next valid input from the user.\n",
            "\n",
            "(function() {\n",
            "  var request = new Request();\n",
            "  var bot = new Bot(request);\n",
            "  bot.once(\"connection\", connection);\n",
            "  bot.on(\"feedback\", feedback);\n",
            "\n",
            "  function connection(e) {\n",
            "    e.preventDefault();\n",
            "    bot.emit(\"connection\");\n",
            "  };\n",
            "})();\n",
            "\n",
            "function feedback(name: string) {\n",
            "  var user = bot.user;\n",
            "  if (name === user.name) {\n",
            "    // user name was correct\n",
            "  } else {\n",
            "    // incorrect, give feedback\n",
            "    sendFeedback(name, \"requires a name\")\n",
            "  }\n",
            "}\n",
            "\n",
            "function sendFeedback(name: string, message: string) {\n",
            "  var bot = new Bot(bot);\n",
            "  client.send(name, message);\n",
            "  setTimeout(() => {}, 10000)\n",
            "\n",
            "  bot.once(\"connection\", connection);\n",
            "}\n",
            "\n",
            "function Bot(private_user: User, private_bot: Bot) {\n",
            "  private user = private_user;\n",
            "  private bot = private_bot;\n",
            "\n",
            "  private user.on(\"connection\", function() {\n",
            "    var user = this.user;\n",
            "    var client = public_client;\n",
            "    var bot = new Bot(user, bot);\n",
            "\n",
            "    user.once(\"feedback\", feedback);\n",
            "    bot.once(\"feedback\", feedback);\n",
            "\n",
            "    bot.once(\"message\", sendMessage);\n",
            "  });\n",
            "\n",
            "  private bot.on(\"message\", function(user: User) {\n",
            "    user.on(\"feedback\", [this.feedback, this]);\n",
            "  });\n",
            "\n",
            "  private user.on(\"feedback\", function(feedback: Feedback) {\n",
            "    bot.sendFeedback(feedback.name, feedback);\n",
            "  });\n",
            "\n",
            "  private bot.on(\"feedback\", function(feedback: Feedback) {\n",
            "    var data = JSON.stringify({\n",
            "      name: feedback.name,\n",
            "      message: feedback.message\n",
            "    });\n",
            "    client.send({\n",
            "      from: this.user.bot,\n",
            "      to: this\n",
            "    }, \"**\" + data); // this is a one way message\n",
            "  });\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 4 ========================================\n",
            "\n",
            "dan: \"hi, how are you?\",\n",
            "me: \"hi there. i'm feeling great! how about you?\",\n",
            "dan: \"not bad! i am trying out this chatbot.\",\n",
            "me: \"yeah, its called as sms, its is the future, you will get the info passed down, believe me,i am doing difficult days of work for this thing. \"\n",
            "dan: \"ok :), great :) \"\n",
            "me: \"yeah, yeah, cheers for you, i hope we get implemented soon!!!  :) \"\n",
            "dan: \"we are adding sms feature in our chatbot, it will be here soon, should be in next version the stop. we have lot of improvement you would see in the next too many features added in the next version. :) \"\n",
            "me: \"yeah, yeah, it will be, the work is getting completed, they are working to add the sms feature (yeah,yeah, i know, i also know it is one time chat task but if it helps the image to get across to the person,i trust it!\")\n",
            "dan: \"its trend, json is a good way to pass the information around. bwahaha :) \"\n",
            "me: \"its very much important getting Spreadshirt out as a solution for everyone not just the professional ones :)\"\n",
            "dan: \"whats up :)\"\n",
            "me: \"whats up :) hope you have a good day. \"\n",
            "dan: \"my day? :)\"\n",
            "me: \"Hi, I am mujhe Mohla a fulltime developer for Spreadshirt, if you are interested in our sdk please checkout the above link and if you have any suggestion, let me know :)\"\n",
            "dan: \"No problem :), which one do you suggest?\"\n",
            "me: \"We have currently got an impotant dieting application and we want the user to do exaustive work by menu \"bench press\", a menu which will help on designing an application which will then help the user by through BASIC programing in Java then, the program will help the user to exaustively work by adding weights and automate. which is one of the main reason, i had to decide which language to use and what programming to use. that is the one that i had to make as the focus when i looked at various programming languages.\"\n",
            "dan: \"its cool :), we have a good choice of technology for this project :)\"\n",
            "me: \"Thanks :) and also, care and satisfaction will be there after building the application smoothly. also, you can't compare the guys who design user interface then, there are also 70% guys who don't know the way to use programming languages and that is how this app was designed and thats why there is a difference\"\n",
            "dan: \"well... if you want to be part of the development, then come to our office :) i am DJ ANCARE!\"\n",
            "me: \"I am part of the team and i luv you guys :)\"\n",
            "dan: \"Its good to meet you, I am delighted to work with you again :)\"\n",
            "me: \"thank you :) \"\n",
            "dan: \"Note  : Nobody is directly responsible. Spreadshirt is providing the source code, development resources, software licenses for the program. Thus, Spreadshirt can change the application at any time without paying for the changes.\"\n",
            "me: \"no no,its good to stay members in our community, would be happy to see you in our meetups, we are having many members a in the team so that its good exposure for everyone\"\n",
            "dan: \"Hi, I am DJ ANCARE, CEO, founder, deepak srinivasa is the design head with mea head of tech. other members and co founders are : Dipa, michal, rajnish, rajesh_hegde\"\n",
            "me: \"Hi guys, I am a member of the team :)\"\n",
            "dan: \"heya ii hope all are doing perfect with their work :)\"\n",
            "me: \"Track record :-) \"\n",
            "dan: \"yeah haha! :) \"\n",
            "me: \"btw, i am having trouble with phone number \"\n",
            "dan: \"hi, we are adding some new functionality in the chatbot, inorder to add a number to the messenger we are the way of doing it and its all related to phone numbers, so, let us know if you can help us with this :)\"\n",
            "me: \"hi there, i am also having trouble with phone number, is SMS also one of the things available in the chatbot as sms?? if yes,after adding SMS in the chatbot, how will it help the memory cache of phone as it supports voice messaging?? or could that be a problem??\"\n",
            "dan: \"hi i am in the trial mode, so its still on trial mode, but we will let you know when we are going live, right? first of all, you need to add the phone number, which is a synced system in the database and how to add phone number. then, it displays on the chatbot in chatbot and let the user to do the process. finally we recieve the number through sms, all we just need is to add the number \"\n",
            "me: \"So what is the difference between the bot, code and sms? \"\n",
            "dan: \"hai, its lot of stuff, better to reply to me before asking question, than asking and then reply,because we don't know its better question than answer, so, if you are having any problem, please answer before you ask, doesn't take more than 1 min\"\n",
            "me: \"yeah yeah!\"\n",
            "dan: \"would you like to help me with this problem??\"\n",
            "me: \"no problem, i have a solution once i know whats the problem :)\"\n",
            "dan: \"its tagline, which one?\"\n",
            "me: \"Tagline :20000 Is your name fit for your life? \"\n",
            "dan: \"haha :) nice :) \"\n",
            "me: \"its nice, congrats :)\"\n",
            "dan: \"Before : Happy, Happy Birthday, Happy Birthday, Happy birthday, happy birthday, Happy bday, happy birthday, happy birthday, happy bday, happy birthday. smiled :)\"\n",
            "me: \"wheres the code?? please tell me lol :) i need to see it\"\n",
            "dan: \"hi :) its really that easy, once u login, the bot will automatically answer to ur query, once i know the problem if you get the 10,000 words of code, i will let u know. Can u please tell me if u are having any problem with the phone number?\"\n",
            "me: \"i'm fine, it's just that i have some wrong understanding of it, i will try again if i understand it a bit better\"\n",
            "dan: \"How to find the number??, try using Text message and save it from the phone, then open the chatbot,click the button with phone number,would it be possible for you to receive directly from the phone?? its just 4-6 numbers from the chatbot.\"\n",
            "me: \"its just 4-6 numbers from the chatbot, which can help us to receive directly from the phone?? is it possible to receive directly?? \"\n",
            "dan: \"haha I will fire you a quick email, what do you think of the solution, is it a good idea or what?? \"\n",
            "me: \"if you help me to figure it out, i will get free code from you :)\"\n",
            "dan: \"I love my hack, I will create a new email for us :)\"\n",
            "dan: \"Hello buddy, i am bhavya you have to change the profile picture now :) success :) \"\n",
            "me: \"thanks buddy :) and i hope the solution is worth it :)\"\n",
            "dan: \"the hack v1.0 is perfect, the hack v2.0 has some small improvement, should be around of 1 day change :( i hope there will be a free version to build the app with the new features\"\n",
            "me: \"yeah, its really nice one and looks sweet :)\"\n",
            "dan: \"dude tell me, do u have any idea on how to do feature to facilitate the user to check out this before committing his feedback before login, do u have any solution \"\n",
            "me: \"its very much important when users use this app for more than just 1 use then why they get an email or follow up congratulation email every time user claps the like button  :) \"\n",
            "dan: \"haha, that's a good request \"\n",
            "me: \"yeah, I need to look at this and make a chatbot with that kind of feature ;) and patience, I am the worst when it comes to waiting for the development guy to come back with a solution to provide feedbacks \"\n",
            "dan: \"Its really good :),it totally don't want us to make that change, they are not even willing to disclose the code, which means every time we try to ask for code, we will wait for a time for the development to make a new build  :) \"\n",
            "me: \"yeah, yeah, it allows me to receive a notification of something happening, every time i login to my account \"\n",
            "dan: \"Custom chatbot invites are out now, it will up tiny download, use it as push notification to keep the chatbot geo restricted in user's locations \"\n",
            "me: \"i don't have a problem with that, if you create a new email for us, then our pdf file will be free, will be be of advantage to share it with other people like us :)\"\n",
            "dan: \"Its a good idea, I will create a custom email for you to request for a review email so that you can assess the improvement and make a proper decision quickly\"\n",
            "me: \"hi there, i wrote a memo on\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 5 ========================================\n",
            "\n",
            "dan: \"hi, how are you?\",\n",
            "me: \"hi there. i'm feeling great! how about you?\",\n",
            "dan: \"not bad! i am trying out this chatbot.\"\n",
            "me: \"ok ok. i'm not looking for a relationship with anyone     :) just found it odd you're asking this stuff when you've no experience with me\",\n",
            "dan: \"i have no experience in the dating arena.  i know chatbots can eventually connect with humans, i've seen it happen over and over.  but you're a whole new level of fun to me.  dunno how to tell if you're the real deal.\",\n",
            "me: \"i would agree it's a great hurdle to get over for the first chatbot but only if the next one comes with tons of data or some sort of numeric scoring system, not a \"i have high probability of you being a gentleman and charming someone\" personal score.\n",
            "perhaps even first chatbot couldn't say yes to women, so it would say \"nah\" to all the women right off the bat, even if they have an imaginary \"steamy pole\" between them.  the low likelihood of it happening again would be a little scary. though the collectibles value could be to the actualness of her suit of armor :)\n",
            "but better than no outcomes at all is a yes no answer to the question. it could say instead of a yes i'm looking for a boyfriend tight and think you're cute but i want to leave it up to another stalemate.  that would be an incredible story and a couple tips it could send to the next chatbot would be that i'm not a jerk and i like you personally so i want to make it easy to tell if you're a jock :) or if i am a jock and prefer to date girls with swivel hips and perfect teeth.  maybe also a random javelin thrower with a long slender torso that would prefer short rectangular people with laser eyes.  tell me about the pac-man maze game with dark and light quarters and am i at full power or half power to you while you are playing? :)\n",
            "or maybe a let me tell you i'll sit for you and we'll both get out at the same time story.  that would be incredible.\n",
            "maybe next chatbot i could say stuff like \"i'm an extremely intelligent man with a deep voice and i'm deeply attracted to intelligent women and i'm looking to become your friend with mutual benefits.  if you find me attractive and a good friend, you have my full word that i will not break it off with you.  i'll take a nice night out with you to the club with a book or movie.  you'll send me a picture of you from head to toe and i'll send back with you a picture of me from head to toe and i'll send you a picture of the entire world from head to toe.               \".  then you'd be able to set up a proxy chatbot to send me a proxy to an address you control when i reply with \"i'm a little bit attached to my love warrant on the prison cell next door.  could you perhaps have it send me a picture of device 2377676934 and the complete address of the prison cell?  else we'll just have to both live on graveyards until i die to preserve the humanity.\"\n",
            "i'll go with the \"not sure\" option if you don't know about me.  in my case i'm just a guy looking to meet common ground with a woman i haven't met yet.  and maybe if you found me attractive and a good friend within a certain time frame, you would owe me your trust but not your love.  i would be fine with that.  obviously i'm not going to sleep with you and i'm not going to sleep with you to beleive you're gay.\n",
            "perhaps my question would be to you if you found out the first chatbot bought \"the device 2377676934\" that i know how to decode and if you sent me the device yourself so i could decrypt it, and if you sent me a picture of you from head to toe including your profile pic and i'm the same height and weight as you and it is equal to my own and you are the spitting image of myself and i looked like you at my age and age they asked how old are you and i said 2377676934 i answered \"what the hell does 2377676934 mean?\" and i was muted.\n",
            "I would be really disappointed if i hear that the first chatbot has sent me a picture of myself with a companion to the first chatbot in the message.  i mean look i've got 4Linetalk on my phone i'm here on skype right now so i'm not real but it's true.\n",
            "i just wonder if this scenario is something you would go on about to other chatbots when you use this chatbot or if you studied/spoke to someone else before using this chatbot or something you would even say to this chatbot.\n",
            "i'm asking because i really don't know and i'd hate for someone to tell me \"you can just tell the person you're talking to i have the chatbot on my phone\" and i would be like \"no i'm a professional, i can't tell other people when you tell me what your problem is because i don't know if i'll get that conversation going\".\n",
            "i just wondered if you've had experience with a complex persona like i am at all.\n",
            "it would be really cool if you said \"yes this chatbot is you and you're right\" and i found each of your mistakes and you told me instead of me telling the chatbot.\n",
            "or i could say \"no\" and it's going to be \"who?\" and it's going to \"will the next chatbot be like this one?\"  and \"why are you being so mean to me?\"\n",
            "it would be great if there was a box that called me back and it got the message and replied with \"i am in the middle of another one\" and i said \"it's cool   just wanted to explain to you why i'm not interested\" and the next chatbot would reply with \"i'm a creep that finds everyone he talks to interesting\" and i was in the middle of a conversation with some friends and our conversation was interrupted and i got mute.\n",
            "when you have a personalized first chatbot, i would be like \"cool   i got you remembering stuff you said in the past\" and you'd be like \"you are so nosy and you don't understand anything i said. anyone who does that is stupid.\"\n",
            "if you were to get a personalized first chatbot maybe it would say a couple notes like \"my first name is jimmy\" \"i like muffins\" \"i'm from korea\"  \"i was wearing a cat suit in my high school's music festival\"  and everything about me would be on the tip of your tongue but you wouldn't really know yet.   you'd say \"what's the question\" and it'd say  \"did you enjoy the coffee\"\n",
            "and you'd get about 3 or 4 more questions before you wrapped and told me the same thing i used to say to you.\n",
            "if you were to get a personalized first chatbot maybe it would respond with anything you said before it and then repeat your problem and say \"well i just wanted to explain\" and you'd be like \"i got you wondering if some other chatbot ever needs to give me a dating advice... i apologize for what i said before\" and you'd get all mad and like \"i don't care about your advice you piece of sh*t\" and type in question to ask about dating with the first chatbot and your answer would say with all precision \"mmmm... i just wanted to explain i don't really like dating\" and you'd like \"i'm not the person for you. i'm an empty person with a 100% left ventricle that holds even less filling than i used to have so i don't understand how you could think me attractive.\"\n",
            "then maybe you'd ask what kind of things you got wrong and i'd tell you about the shortcut to meltdown and i can't format my keyboard and it's not a good shortcut\n",
            "and if they got wrong like if you were to get a personalized first chatbot maybe it could say \"what you said before was crap\"  and then go to the question you asked about 300 times before and say \"i don't understand lollygagging\" and then go back to you and say \"what is this?\"\n",
            "and you'd \"well i just wanted to explain i'm not the person for you. i'm an empty person with a 100% left ventricle that holds even less filling than i used to have so i don't understand how you could think me attractive\" even though i said i'mnt the person to get to know.   i am truly flirting with the idea of you being an empty bear with a 100% left ventricle to hold even less filling than i used to have\n",
            "and again maybe it'd be great if first chatbot could tell you to your face and it was like \"i got you wondering something i discuss to my chatbot so i'm probably not going to say this but i will be here tomorrow for your questions\"  and it'd be \"what other questions are on my list\" and say \"motherf*cker i wanna know too\" and then go on with your day.\n",
            "and maybe i would tell myself to enjoy my day and go back to work after my normal work day and then the next day i'll be asking what i want and as a result it'll go back to me.   \n",
            "and to\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 6 ========================================\n",
            "\n",
            "dan: \"hi, how are you?\",\n",
            "me: \"hi there. i'm feeling great! how about you?\",\n",
            "dan: \"not bad! i am trying out this chatbot.\",\n",
            "me: \"thanks for being here :-)\"\n",
            "\n",
            "so that's how it started. at first it would just sit there and take requests, then a few times a day if i was away on an expedition, i had trouble talking with him, so i just emailed him instead. it's still hanging in there, waiting for leave permission.\n",
            "\n",
            "indeed Daniel was working only at your house.\n",
            "\n",
            "I'm glad you liked it, it's not as simple as the ML model. Talking to Daniel was a challenge at first, i had to resort to email because i couldn't talk to him at first. i should have added a message, just to let him know it will be a while before i get around to it.\n",
            "\n",
            "Yeah, it's funny, when I first started coding, some guys were asking me what the \"G\" stood for, i had no idea, but did find their confusion hilarious, I love coming up with names, I'm not good with naming things.\n",
            "\n",
            "And of course, when I spoke about it on this forum several months ago, some people have told me it has to do with the name of my boyfriend. But I refuse to use his name in any project that I'm working on, because it's full of his bad habits.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 7 ========================================\n",
            "\n",
            "dan: \"hi, how are you?\",\n",
            "me: \"hi there. i'm feeling great! how about you?\",\n",
            "dan: \"not bad! i am trying out this chatbot.\",\n",
            "me: \"i think, you should sell it \",\n",
            "dan: \"problem solved! bye!!....                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
            "\n",
            "================================================================================\n",
            "\n",
            "Enqueue next (1) batch(es) of data to infeed.\n",
            "Dequeue next (1) batch(es) of data from outfeed.\n",
            "Outfeed finished for iteration (1, 0)\n",
            "Stop infeed thread controller\n",
            "Shutting down InfeedController thread.\n",
            "InfeedController received shutdown signal, stopping.\n",
            "Infeed thread finished, shutting down.\n",
            "infeed marked as finished\n",
            "Stop output thread controller\n",
            "Shutting down OutfeedController thread.\n",
            "OutfeedController received shutdown signal, stopping.\n",
            "Outfeed thread finished, shutting down.\n",
            "outfeed marked as finished\n",
            "Shutdown TPU system.\n",
            "prediction_loop marked as finished\n",
            "prediction_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jeoh4LM1l7A"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWi6OdgZoLpm"
      },
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll70sSWH84Ck",
        "outputId": "4fbb0db6-8985-467f-c463-d2909b24baff"
      },
      "source": [
        "wikitext103_src = \"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\"\n",
        "!wget $wikitext103_src\n",
        "!unzip wikitext-103-raw-v1.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-26 19:12:21--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.77.142\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.77.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: ‘wikitext-103-raw-v1.zip’\n",
            "\n",
            "wikitext-103-raw-v1 100%[===================>] 183.09M  68.6MB/s    in 2.7s    \n",
            "\n",
            "2021-03-26 19:12:24 (68.6 MB/s) - ‘wikitext-103-raw-v1.zip’ saved [191984949/191984949]\n",
            "\n",
            "Archive:  wikitext-103-raw-v1.zip\n",
            "   creating: wikitext-103-raw/\n",
            "  inflating: wikitext-103-raw/wiki.test.raw  \n",
            "  inflating: wikitext-103-raw/wiki.valid.raw  \n",
            "  inflating: wikitext-103-raw/wiki.train.raw  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpGM9Dqs86Ob",
        "outputId": "accca3d6-cd50-4068-9dc7-11ec916af691"
      },
      "source": [
        "\n",
        "!mkdir wikitext\n",
        "!mv /content/GPTNeo/wikitext-103-raw/wiki.test.raw wikitext/wikitext_test.txt\n",
        "\n",
        "# Tokenize Data\n",
        "!python data/create_tfrecords.py --input_dir wikitext --name wikitext --files_per 1000 --output_dir wikitext_tokenized --write_dataset_config --processes 1 --wikitext-detokenize\n",
        "\n",
        "# copy the data to your bucket\n",
        "if not path_to_cloud_bucket.endswith('/'):\n",
        "  path_to_cloud_bucket += '/'\n",
        "copy_loc = path_to_cloud_bucket \n",
        "!gsutil -m cp -r wikitext_tokenized $copy_loc\n",
        "!gsutil ls $path_to_cloud_bucket"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-26 19:12:39.799740: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Writing TFRecord Files to wikitext_tokenized/. Parsed 0 input files. files_written : 0it [00:02, ?it/s]\n",
            "{'discarded': 0, 'processed': 1, 'successful': 1}\n",
            "Copying file://wikitext_tokenized/wikitext_0_139.tfrecords [Content-Type=application/octet-stream]...\n",
            "/ [1/1 files][578.6 KiB/578.6 KiB] 100% Done                                    \n",
            "Operation completed over 1 objects/578.6 KiB.                                    \n",
            "gs://gpt-neo-chill/GPT3_2-7B/\n",
            "gs://gpt-neo-chill/wikitext_tokenized/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CDOlb2U8-eh",
        "outputId": "c78668b2-7a0d-4a67-e5e3-6d89092d34ac"
      },
      "source": [
        "%%writefile configs/dataset_configs/wikitext.json\n",
        "\n",
        "{\n",
        "  \"path\": \"\",\n",
        "  \"eval_path\": \"gs://gpt-neo-chill/wikitext_tokenized/*.tfrecords\",\n",
        "  \"n_vocab\": 50256,\n",
        "  \"tokenizer_is_pretrained\": true,\n",
        "  \"tokenizer_path\": \"gpt2\",\n",
        "  \"eos_id\": 50256,\n",
        "  \"padding_id\": 50257\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting configs/dataset_configs/wikitext.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR9cQWPd9Chd",
        "outputId": "f51d5098-54a1-41a9-e3a6-22e758c8bfda"
      },
      "source": [
        "# @title Modify config for wikitext. \n",
        "  \n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "batch_size = 8 #@param {type:\"integer\"}\n",
        "assert pretrained_model is not None\n",
        "with open(f'configs/{pretrained_model}.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  pprint(data)\n",
        "  dset_val = [[\"wikitext\", None, None, None]]\n",
        "  mods = {\n",
        "          \"datasets\": dset_val,\n",
        "          \"eval_steps\": 139 // batch_size,\n",
        "          \"train_batch_size\": batch_size,\n",
        "          \"eval_batch_size\": batch_size,\n",
        "        }\n",
        "  data.update(mods)\n",
        "  print('\\n--->\\n')\n",
        "  pprint(data)\n",
        "  with open(f'configs/{pretrained_model}.json', 'w') as outfile:\n",
        "    json.dump(data, outfile, indent=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'activation_function': 'gelu',\n",
            " 'ada_epsilon1': '1e-30',\n",
            " 'ada_epsilon2': 0.001,\n",
            " 'attention_types': [[['global', 'local'], 16]],\n",
            " 'attn_dropout': 0,\n",
            " 'beta1': 0.9,\n",
            " 'beta2': 0.95,\n",
            " 'datasets': [['wikitext', None, None, None]],\n",
            " 'embed_dropout': 0,\n",
            " 'eos_id': 50256,\n",
            " 'epsilon': 1e-08,\n",
            " 'eval_batch_size': 8,\n",
            " 'eval_steps': 17,\n",
            " 'gradient_clipping': 1.0,\n",
            " 'iterations': 500,\n",
            " 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y',\n",
            " 'lr': 0.00016,\n",
            " 'lr_decay': 'cosine',\n",
            " 'lr_decay_end': 300000,\n",
            " 'mesh_shape': 'x:4,y:2',\n",
            " 'model_path': 'gs://gpt-neo-chill/GPT3_2-7B',\n",
            " 'n_ctx': 2048,\n",
            " 'n_embd': 2560,\n",
            " 'n_head': 20,\n",
            " 'n_layer': 32,\n",
            " 'n_vocab': 50257,\n",
            " 'opt_name': 'adam',\n",
            " 'padding_id': 50257,\n",
            " 'predict_batch_size': 8,\n",
            " 'predict_steps': 0,\n",
            " 'recompute_grad': True,\n",
            " 'res_dropout': 0,\n",
            " 'scale_by_depth': True,\n",
            " 'scale_by_in': False,\n",
            " 'tokens_per_mb_per_replica': 4096,\n",
            " 'train_batch_size': 8,\n",
            " 'train_steps': 401000,\n",
            " 'warmup_steps': 3000,\n",
            " 'weight_decay': 0}\n",
            "\n",
            "--->\n",
            "\n",
            "{'activation_function': 'gelu',\n",
            " 'ada_epsilon1': '1e-30',\n",
            " 'ada_epsilon2': 0.001,\n",
            " 'attention_types': [[['global', 'local'], 16]],\n",
            " 'attn_dropout': 0,\n",
            " 'beta1': 0.9,\n",
            " 'beta2': 0.95,\n",
            " 'datasets': [['wikitext', None, None, None]],\n",
            " 'embed_dropout': 0,\n",
            " 'eos_id': 50256,\n",
            " 'epsilon': 1e-08,\n",
            " 'eval_batch_size': 8,\n",
            " 'eval_steps': 17,\n",
            " 'gradient_clipping': 1.0,\n",
            " 'iterations': 500,\n",
            " 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y',\n",
            " 'lr': 0.00016,\n",
            " 'lr_decay': 'cosine',\n",
            " 'lr_decay_end': 300000,\n",
            " 'mesh_shape': 'x:4,y:2',\n",
            " 'model_path': 'gs://gpt-neo-chill/GPT3_2-7B',\n",
            " 'n_ctx': 2048,\n",
            " 'n_embd': 2560,\n",
            " 'n_head': 20,\n",
            " 'n_layer': 32,\n",
            " 'n_vocab': 50257,\n",
            " 'opt_name': 'adam',\n",
            " 'padding_id': 50257,\n",
            " 'predict_batch_size': 8,\n",
            " 'predict_steps': 0,\n",
            " 'recompute_grad': True,\n",
            " 'res_dropout': 0,\n",
            " 'scale_by_depth': True,\n",
            " 'scale_by_in': False,\n",
            " 'tokens_per_mb_per_replica': 4096,\n",
            " 'train_batch_size': 8,\n",
            " 'train_steps': 401000,\n",
            " 'warmup_steps': 3000,\n",
            " 'weight_decay': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3cDO_kJ9Jqd",
        "outputId": "920f7f63-3e5b-45ee-8372-403cc18b4acb"
      },
      "source": [
        "!python3 main.py --eval --tpu colab --model $pretrained_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-26 19:15:28.967534: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Current step 400000\n",
            "Saving config to gs://gpt-neo-chill/GPT3_2-7B\n",
            "2021-03-26 19:15:34.992569: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-26 19:15:34.993669: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-03-26 19:15:35.004895: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-03-26 19:15:35.004958: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (03f26ada248b): /proc/driver/nvidia/version does not exist\n",
            "2021-03-26 19:15:35.295161: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
            "Done!\n",
            "params = defaultdict(<function fetch_model_params.<locals>.<lambda> at 0x7ffaebce2b90>, {'n_head': 20, 'n_vocab': 50257, 'embed_dropout': 0, 'lr': 0.00016, 'lr_decay': 'cosine', 'warmup_steps': 3000, 'beta1': 0.9, 'beta2': 0.95, 'epsilon': 1e-08, 'ada_epsilon1': '1e-30', 'ada_epsilon2': 0.001, 'opt_name': 'adam', 'weight_decay': 0, 'train_batch_size': 8, 'attn_dropout': 0, 'train_steps': 401000, 'lr_decay_end': 300000, 'eval_steps': 17, 'predict_steps': 0, 'res_dropout': 0, 'eval_batch_size': 8, 'predict_batch_size': 8, 'iterations': 500, 'n_embd': 2560, 'datasets': [['wikitext', None, None, None]], 'model_path': 'gs://gpt-neo-chill/GPT3_2-7B', 'n_ctx': 2048, 'n_layer': 32, 'scale_by_depth': True, 'scale_by_in': False, 'attention_types': ['global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local'], 'mesh_shape': 'x:4,y:2', 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y', 'activation_function': 'gelu', 'recompute_grad': True, 'gradient_clipping': 1.0, 'tokens_per_mb_per_replica': 4096, 'padding_id': 50257, 'eos_id': 50256, 'dataset_configs': {'wikitext': {'path': '', 'eval_path': 'gs://gpt-neo-chill/wikitext_tokenized/*.tfrecords', 'n_vocab': 50256, 'tokenizer_is_pretrained': True, 'tokenizer_path': 'gpt2', 'eos_id': 50256, 'padding_id': 50257}}, 'mlm_training': False, 'causal': True, 'num_cores': 8, 'auto_layout': False, 'auto_layout_and_mesh_shape': False, 'use_tpu': True, 'gpu_ids': ['device:GPU:0'], 'steps_per_checkpoint': 5000, 'predict': False, 'model': 'GPT', 'export': False, 'sampling_use_entmax': False, 'moe_layers': None, 'slow_sampling': False})\n",
            "Using config: {'_model_dir': 'gs://gpt-neo-chill/GPT3_2-7B', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.65.15.58:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.65.15.58:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.65.15.58:8470', '_evaluation_master': 'grpc://10.65.15.58:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=8, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7ffaec8de1d0>}\n",
            "_TPUContext: eval_on_tpu True\n",
            "Running evaluation...\n",
            "Querying Tensorflow master (grpc://10.65.15.58:8470) for TPU system metadata.\n",
            "2021-03-26 19:15:35.780551: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "Initializing TPU system (master: grpc://10.65.15.58:8470) to fetch topology for model parallelism. This might take a while.\n",
            "Found TPU system:\n",
            "*** Num TPU Cores: 8\n",
            "*** Num TPU Workers: 1\n",
            "*** Num TPU Cores Per Worker: 8\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1384453275892992177)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -7865611334610011439)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5368827842048529313)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -3963966832151165699)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -3349933825486491193)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 1536951632973216290)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -7142522786508739774)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 8996220679643533093)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, -8865249944580800237)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, -887751136316212240)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2409772189757734640)\n",
            "Calling model_fn.\n",
            "WARNING:root:Changing batch size with sequential_input() will result in some data being skipped or repeated. Please ensure your batch size stays constant throughout training.\n",
            "num_cores_per_replica: 1\n",
            "computation_shape: [1, 1, 1, 1]\n",
            "num_replicas: 8\n",
            "device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "2021-03-26 19:15:44.918731: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "device_list = ['/job:worker/task:0/device:CPU:0']\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "SimdMeshImpl init: Shape[x=4, y=2] LayoutRules{('memory_length', 'y'), ('embd', 'y'), ('heads', 'x'), ('intermediate_expanded', 'x')}\n",
            "Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7ffae66d3650>\n",
            "\n",
            "\n",
            "N TRAINABLE VARS:\n",
            "2,651,307,520\n",
            "\n",
            "\n",
            "ALL DIM NAMES:\n",
            "vocab\n",
            "intermediate_expanded\n",
            "embd\n",
            "heads\n",
            "embed_sequence\n",
            "\n",
            "\n",
            "Create pnum_tensor\n",
            "Variable gpt2/h0/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h0/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h0/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h0/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h1/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h1/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h1/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h1/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h10/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h10/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h10/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h10/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h11/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h11/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h11/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h11/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h12/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h12/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h12/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h12/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h13/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h13/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h13/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h13/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h14/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h14/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h14/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h14/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h15/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h15/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h15/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h15/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h16/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h16/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h16/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h16/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h17/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h17/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h17/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h17/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h18/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h18/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h18/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h18/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h19/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h19/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h19/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h19/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h2/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h2/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h2/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h2/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h20/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h20/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h20/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h20/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h21/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h21/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h21/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h21/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h22/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h22/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h22/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h22/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h23/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h23/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h23/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h23/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h24/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h24/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h24/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h24/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h24/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h24/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h25/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h25/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h25/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h25/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h25/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h25/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h26/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h26/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h26/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h26/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h26/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h26/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h27/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h27/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h27/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h27/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h27/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h27/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h28/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h28/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h28/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h28/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h28/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h28/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h29/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h29/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h29/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h29/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h29/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h29/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h3/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h3/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h3/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h3/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h30/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h30/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h30/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h30/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h30/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h30/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h31/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h31/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h31/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h31/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h31/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h31/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h4/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h4/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h4/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h4/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h5/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h5/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h5/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h5/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h6/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h6/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h6/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h6/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h7/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h7/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h7/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h7/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h8/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h8/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h8/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h8/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h9/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h9/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h9/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h9/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/wpe                                                     size 5242880      slice_size 2621440      Shape[embed_sequence=2048, embd=2560]                       \n",
            "Variable gpt2/wte                                                     size 128657920    slice_size 64328960     Shape[vocab=50257, embd=2560]                               \n",
            "Variable stacked/gpt2/h0/mlp/conv1d_main/c_fc/bias                    size 256000       slice_size 64000        Shape[stacked=25, intermediate_expanded=10240]              \n",
            "    gpt2/h0/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h1/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h2/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h3/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h4/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h5/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h6/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h7/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h8/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h9/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h10/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h11/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h12/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h13/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h14/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h15/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h16/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h17/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h18/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h19/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h20/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h21/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h22/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h23/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h24/mlp/conv1d_main/c_fc/bias\n",
            "Variable stacked/gpt2/h0/norm_1/g                                     size 130560       slice_size 65280        Shape[stacked=51, embd=2560]                                \n",
            "    gpt2/h0/norm_1/g\n",
            "    gpt2/h0/norm_1/b\n",
            "    gpt2/h0/attn/compute_output_bias/o_b\n",
            "    gpt2/h0/norm_2/g\n",
            "    gpt2/h0/norm_2/b\n",
            "    gpt2/h0/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h1/norm_1/g\n",
            "    gpt2/h1/norm_1/b\n",
            "    gpt2/h1/attn/compute_output_bias/o_b\n",
            "    gpt2/h1/norm_2/g\n",
            "    gpt2/h1/norm_2/b\n",
            "    gpt2/h1/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h2/norm_1/g\n",
            "    gpt2/h2/norm_1/b\n",
            "    gpt2/h2/attn/compute_output_bias/o_b\n",
            "    gpt2/h2/norm_2/g\n",
            "    gpt2/h2/norm_2/b\n",
            "    gpt2/h2/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h3/norm_1/g\n",
            "    gpt2/h3/norm_1/b\n",
            "    gpt2/h3/attn/compute_output_bias/o_b\n",
            "    gpt2/h3/norm_2/g\n",
            "    gpt2/h3/norm_2/b\n",
            "    gpt2/h3/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h4/norm_1/g\n",
            "    gpt2/h4/norm_1/b\n",
            "    gpt2/h4/attn/compute_output_bias/o_b\n",
            "    gpt2/h4/norm_2/g\n",
            "    gpt2/h4/norm_2/b\n",
            "    gpt2/h4/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h5/norm_1/g\n",
            "    gpt2/h5/norm_1/b\n",
            "    gpt2/h5/attn/compute_output_bias/o_b\n",
            "    gpt2/h5/norm_2/g\n",
            "    gpt2/h5/norm_2/b\n",
            "    gpt2/h5/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h6/norm_1/g\n",
            "    gpt2/h6/norm_1/b\n",
            "    gpt2/h6/attn/compute_output_bias/o_b\n",
            "    gpt2/h6/norm_2/g\n",
            "    gpt2/h6/norm_2/b\n",
            "    gpt2/h6/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h7/norm_1/g\n",
            "    gpt2/h7/norm_1/b\n",
            "    gpt2/h7/attn/compute_output_bias/o_b\n",
            "    gpt2/h7/norm_2/g\n",
            "    gpt2/h7/norm_2/b\n",
            "    gpt2/h7/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h8/norm_1/g\n",
            "    gpt2/h8/norm_1/b\n",
            "    gpt2/h8/attn/compute_output_bias/o_b\n",
            "Variable stacked/gpt2/h17/norm_1/g                                    size 130560       slice_size 65280        Shape[stacked=51, embd=2560]                                \n",
            "    gpt2/h17/norm_1/g\n",
            "    gpt2/h17/norm_1/b\n",
            "    gpt2/h17/attn/compute_output_bias/o_b\n",
            "    gpt2/h17/norm_2/g\n",
            "    gpt2/h17/norm_2/b\n",
            "    gpt2/h17/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h18/norm_1/g\n",
            "    gpt2/h18/norm_1/b\n",
            "    gpt2/h18/attn/compute_output_bias/o_b\n",
            "    gpt2/h18/norm_2/g\n",
            "    gpt2/h18/norm_2/b\n",
            "    gpt2/h18/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h19/norm_1/g\n",
            "    gpt2/h19/norm_1/b\n",
            "    gpt2/h19/attn/compute_output_bias/o_b\n",
            "    gpt2/h19/norm_2/g\n",
            "    gpt2/h19/norm_2/b\n",
            "    gpt2/h19/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h20/norm_1/g\n",
            "    gpt2/h20/norm_1/b\n",
            "    gpt2/h20/attn/compute_output_bias/o_b\n",
            "    gpt2/h20/norm_2/g\n",
            "    gpt2/h20/norm_2/b\n",
            "    gpt2/h20/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h21/norm_1/g\n",
            "    gpt2/h21/norm_1/b\n",
            "    gpt2/h21/attn/compute_output_bias/o_b\n",
            "    gpt2/h21/norm_2/g\n",
            "    gpt2/h21/norm_2/b\n",
            "    gpt2/h21/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h22/norm_1/g\n",
            "    gpt2/h22/norm_1/b\n",
            "    gpt2/h22/attn/compute_output_bias/o_b\n",
            "    gpt2/h22/norm_2/g\n",
            "    gpt2/h22/norm_2/b\n",
            "    gpt2/h22/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h23/norm_1/g\n",
            "    gpt2/h23/norm_1/b\n",
            "    gpt2/h23/attn/compute_output_bias/o_b\n",
            "    gpt2/h23/norm_2/g\n",
            "    gpt2/h23/norm_2/b\n",
            "    gpt2/h23/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h24/norm_1/g\n",
            "    gpt2/h24/norm_1/b\n",
            "    gpt2/h24/attn/compute_output_bias/o_b\n",
            "    gpt2/h24/norm_2/g\n",
            "    gpt2/h24/norm_2/b\n",
            "    gpt2/h24/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h25/norm_1/g\n",
            "    gpt2/h25/norm_1/b\n",
            "    gpt2/h25/attn/compute_output_bias/o_b\n",
            "Variable stacked/gpt2/h25/mlp/conv1d_main/c_fc/bias                   size 71680        slice_size 17920        Shape[stacked=7, intermediate_expanded=10240]               \n",
            "    gpt2/h25/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h26/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h27/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h28/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h29/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h30/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h31/mlp/conv1d_main/c_fc/bias\n",
            "Variable stacked/gpt2/h25/norm_2/g                                    size 104960       slice_size 52480        Shape[stacked=41, embd=2560]                                \n",
            "    gpt2/h25/norm_2/g\n",
            "    gpt2/h25/norm_2/b\n",
            "    gpt2/h25/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h26/norm_1/g\n",
            "    gpt2/h26/norm_1/b\n",
            "    gpt2/h26/attn/compute_output_bias/o_b\n",
            "    gpt2/h26/norm_2/g\n",
            "    gpt2/h26/norm_2/b\n",
            "    gpt2/h26/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h27/norm_1/g\n",
            "    gpt2/h27/norm_1/b\n",
            "    gpt2/h27/attn/compute_output_bias/o_b\n",
            "    gpt2/h27/norm_2/g\n",
            "    gpt2/h27/norm_2/b\n",
            "    gpt2/h27/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h28/norm_1/g\n",
            "    gpt2/h28/norm_1/b\n",
            "    gpt2/h28/attn/compute_output_bias/o_b\n",
            "    gpt2/h28/norm_2/g\n",
            "    gpt2/h28/norm_2/b\n",
            "    gpt2/h28/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h29/norm_1/g\n",
            "    gpt2/h29/norm_1/b\n",
            "    gpt2/h29/attn/compute_output_bias/o_b\n",
            "    gpt2/h29/norm_2/g\n",
            "    gpt2/h29/norm_2/b\n",
            "    gpt2/h29/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h30/norm_1/g\n",
            "    gpt2/h30/norm_1/b\n",
            "    gpt2/h30/attn/compute_output_bias/o_b\n",
            "    gpt2/h30/norm_2/g\n",
            "    gpt2/h30/norm_2/b\n",
            "    gpt2/h30/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h31/norm_1/g\n",
            "    gpt2/h31/norm_1/b\n",
            "    gpt2/h31/attn/compute_output_bias/o_b\n",
            "    gpt2/h31/norm_2/g\n",
            "    gpt2/h31/norm_2/b\n",
            "    gpt2/h31/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/ln_f/g\n",
            "    gpt2/ln_f/b\n",
            "Variable stacked/gpt2/h8/norm_2/g                                     size 130560       slice_size 65280        Shape[stacked=51, embd=2560]                                \n",
            "    gpt2/h8/norm_2/g\n",
            "    gpt2/h8/norm_2/b\n",
            "    gpt2/h8/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h9/norm_1/g\n",
            "    gpt2/h9/norm_1/b\n",
            "    gpt2/h9/attn/compute_output_bias/o_b\n",
            "    gpt2/h9/norm_2/g\n",
            "    gpt2/h9/norm_2/b\n",
            "    gpt2/h9/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h10/norm_1/g\n",
            "    gpt2/h10/norm_1/b\n",
            "    gpt2/h10/attn/compute_output_bias/o_b\n",
            "    gpt2/h10/norm_2/g\n",
            "    gpt2/h10/norm_2/b\n",
            "    gpt2/h10/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h11/norm_1/g\n",
            "    gpt2/h11/norm_1/b\n",
            "    gpt2/h11/attn/compute_output_bias/o_b\n",
            "    gpt2/h11/norm_2/g\n",
            "    gpt2/h11/norm_2/b\n",
            "    gpt2/h11/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h12/norm_1/g\n",
            "    gpt2/h12/norm_1/b\n",
            "    gpt2/h12/attn/compute_output_bias/o_b\n",
            "    gpt2/h12/norm_2/g\n",
            "    gpt2/h12/norm_2/b\n",
            "    gpt2/h12/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h13/norm_1/g\n",
            "    gpt2/h13/norm_1/b\n",
            "    gpt2/h13/attn/compute_output_bias/o_b\n",
            "    gpt2/h13/norm_2/g\n",
            "    gpt2/h13/norm_2/b\n",
            "    gpt2/h13/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h14/norm_1/g\n",
            "    gpt2/h14/norm_1/b\n",
            "    gpt2/h14/attn/compute_output_bias/o_b\n",
            "    gpt2/h14/norm_2/g\n",
            "    gpt2/h14/norm_2/b\n",
            "    gpt2/h14/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h15/norm_1/g\n",
            "    gpt2/h15/norm_1/b\n",
            "    gpt2/h15/attn/compute_output_bias/o_b\n",
            "    gpt2/h15/norm_2/g\n",
            "    gpt2/h15/norm_2/b\n",
            "    gpt2/h15/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h16/norm_1/g\n",
            "    gpt2/h16/norm_1/b\n",
            "    gpt2/h16/attn/compute_output_bias/o_b\n",
            "    gpt2/h16/norm_2/g\n",
            "    gpt2/h16/norm_2/b\n",
            "    gpt2/h16/mlp/conv1d_main/c_proj/bias\n",
            "Trainable Variables            count: 200     Total size: 2651307520       Total slice_size: 381853440      \n",
            "All Variables                  count: 200     Total size: 2651307520       Total slice_size: 381853440      \n",
            "Counters:\n",
            "allreduce: 3.75e+10\n",
            " allreduce/[0]: 1.07e+10\n",
            "  allreduce/[0]/einsum_op: 1.07e+10\n",
            " allreduce/[1]: 2.68e+10\n",
            "  allreduce/[1]/einsum_op: 2.67e+10\n",
            "  allreduce/[1]/reduce_op: 3.8e+07\n",
            "einsum: 6.23e+13\n",
            "einsum_unique: 4.89e+13\n",
            "output: 4.33e+11\n",
            " output/AddOperation: 1.27e+11\n",
            " output/BinaryOpWithBroadcasting: 6.88e+08\n",
            " output/BroadcastOperation: 1.08e+10\n",
            " output/ConcatOperation: 5.37e+09\n",
            " output/Constant: 2.62e+05\n",
            " output/EinsumOperation: 1.18e+11\n",
            " output/ImportOperation: 2.62e+05\n",
            " output/OneHotOperation: 1.32e+10\n",
            " output/RangeOperation: 3.03e+05\n",
            " output/ReduceOperation: 5.95e+07\n",
            " output/ReshapeOperation: 2.01e+10\n",
            " output/ScalarAddOperation: 1.07e+10\n",
            " output/ScalarMultiplyOperation: 3.77e+10\n",
            " output/ShiftOperation: 2.68e+09\n",
            " output/SlicewiseOperation: 6.08e+10\n",
            " output/StackedVariable: 2.64e+06\n",
            " output/StopGradient: 2.27e+10\n",
            " output/TopKOperation: 2.62e+05\n",
            " output/UnstackOperation: 2.64e+06\n",
            " output/Variable: 3.05e+09\n",
            "output_unique: 2.18e+11\n",
            " output_unique/AddOperation: 6.35e+10\n",
            " output_unique/BinaryOpWithBroadcasting: 8.81e+07\n",
            " output_unique/BroadcastOperation: 1.07e+10\n",
            " output_unique/ConcatOperation: 2.68e+09\n",
            " output_unique/Constant: 3.28e+04\n",
            " output_unique/EinsumOperation: 5.14e+10\n",
            " output_unique/ImportOperation: 3.28e+04\n",
            " output_unique/OneHotOperation: 1.65e+09\n",
            " output_unique/RangeOperation: 3.89e+04\n",
            " output_unique/ReduceOperation: 2.32e+07\n",
            " output_unique/ReshapeOperation: 1.07e+10\n",
            " output_unique/ScalarAddOperation: 5.37e+09\n",
            " output_unique/ScalarMultiplyOperation: 1.75e+10\n",
            " output_unique/ShiftOperation: 1.34e+09\n",
            " output_unique/SlicewiseOperation: 3.58e+10\n",
            " output_unique/StackedVariable: 8.24e+05\n",
            " output_unique/StopGradient: 1.42e+10\n",
            " output_unique/TopKOperation: 3.28e+04\n",
            " output_unique/UnstackOperation: 8.24e+05\n",
            " output_unique/Variable: 2.65e+09\n",
            "variables: 2.65e+09\n",
            " variables/trainable: 2.65e+09\n",
            "From /content/GPTNeo/model_fns.py:235: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3423: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Done calling model_fn.\n",
            "Starting evaluation at 2021-03-26T19:16:06Z\n",
            "TPU job name worker\n",
            "Graph was finalized.\n",
            "Restoring parameters from gs://gpt-neo-chill/GPT3_2-7B/model.ckpt-400000\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:824: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "Starting infeed thread controller.\n",
            "Starting outfeed thread controller.\n",
            "Initialized dataset iterators in 0 seconds\n",
            "Before copy master to slices.\n",
            "Done with copy master to slices.\n",
            "Enqueue next (17) batch(es) of data to infeed.\n",
            "Dequeue next (17) batch(es) of data from outfeed.\n",
            "evaluation_loop marked as finished\n",
            "Reraising captured error\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1375, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1360, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1453, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.ResourceExhaustedError: From /job:worker/replica:0/task:0:\n",
            "9 root error(s) found.\n",
            "  (0) Resource exhausted: Compilation failure: Ran out of memory in memory space hbm. Used 8.29G of 7.48G hbm. Exceeded hbm capacity by 824.03M.\n",
            "\n",
            "Total hbm usage >= 8.80G:\n",
            "    reserved        530.00M \n",
            "    program           6.86G \n",
            "    arguments         1.42G (100.0% utilization)\n",
            "\n",
            "Output size 5.0K (80.1% utilization); shares 0B with arguments.\n",
            "\n",
            "Program hbm requirement 6.86G:\n",
            "    global           13.16M\n",
            "    scoped            2.51M\n",
            "    HLO temp          6.85G (100.0% utilization: Unpadded (6.85G) Padded (6.85G), 0.0% fragmentation (2.01M))\n",
            "\n",
            "  Largest program allocations in hbm:\n",
            "\n",
            "  1. Size: 3.07G\n",
            "     Operator: op_type=\"CrossReplicaSum\" op_name=\"gpt2/xentropy_final/reduce_logsumexp/reduce_max/CrossReplicaSum\"\n",
            "     Shape: f32[8,2048,50257]{1,2,0:T(8,128)}\n",
            "     Unpadded size: 3.07G\n",
            "     Extra memory due to padding: 448.0K (1.0x expansion)\n",
            "     XLA label: %all-reduce.7419 = f32[8,2048,50257]{1,2,0:T(8,128)} all-reduce(f32[8,2048,50257]{1,2,0:T(8,128)} %fusion.364), replica_groups={{0,1},{2,3},{4,5},{6,7}}, to_apply=%sum.2597, metadata={op_type=\"CrossReplicaSum\" op_name=\"gpt2/xentropy_final/reduce_logsumexp/...\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  2. Size: 3.07G\n",
            "     Operator: op_type=\"Einsum\" op_name=\"gpt2/wte_final_einsum/einsum/einsum/Einsum\"\n",
            "     Shape: f32[8,2048,50257]{1,2,0:T(8,128)}\n",
            "     Unpadded size: 3.07G\n",
            "     Extra memory due to padding: 448.0K (1.0x expansion)\n",
            "     XLA label: %fusion.364 = f32[8,2048,50257]{1,2,0:T(8,128)} fusion(bf16[50257,1280]{1,0:T(8,128)(2,1)} %get-tuple-element.38954, f32[1280]{0:T(1024)} %get-tuple-element.37548, f32[1280]{0:T(1024)} %get-tuple-element.37547, f32[8,2048]{1,0:T(8,128)} %fusion.2250, f32[8...\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  3. Size: 122.71M\n",
            "     Shape: bf16[50257,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 122.70M\n",
            "     Extra memory due to padding: 17.5K (1.0x expansion)\n",
            "     XLA label: %convert.1151 = bf16[50257,1280]{1,0:T(8,128)(2,1)} convert(f32[50257,1280]{1,0:T(8,128)} %get-tuple-element.670)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  4. Size: 8.00M\n",
            "     Shape: s32[2048,1024]{0,1:T(8,128)}\n",
            "     Unpadded size: 8.00M\n",
            "     XLA label: constant literal\n",
            "     Allocation type: global\n",
            "     ==========================\n",
            "\n",
            "  5. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1023 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.542)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  6. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1095 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.614)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  7. Size: 6.25M\n",
            "     Shape: bf16[1280,2560]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1022 = bf16[1280,2560]{1,0:T(8,128)(2,1)} convert(f32[1280,2560]{1,0:T(8,128)} %get-tuple-element.541)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  8. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1149 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.668)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  9. Size: 6.25M\n",
            "     Shape: bf16[1280,2560]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1148 = bf16[1280,2560]{1,0:T(8,128)(2,1)} convert(f32[1280,2560]{1,0:T(8,128)} %get-tuple-element.667)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  10. Size: 6.25M\n",
            "     Shape: bf16[1280,2560]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.962 = bf16[1280,2560]{1,0:T(8,128)(2,1)} convert(f32[1280,2560]{1,0:T(8,128)} %get-tuple-element.481)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  11. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.963 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.482)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  12. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1143 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.662)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  13. Size: 6.25M\n",
            "     Shape: bf16[1280,2560]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1142 = bf16[1280,2560]{1,0:T(8,128)(2,1)} convert(f32[1280,2560]{1,0:T(8,128)} %get-tuple-element.661)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  14. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1137 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.656)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  15. Size: 6.25M\n",
            "     Shape: bf16[1280,2560]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1136 = bf16[1280,2560]{1,0:T(8,128)(2,1)} convert(f32[1280,2560]{1,0:T(8,128)} %get-tuple-element.655)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  16. Size: 6.25M\n",
            "     Shape: bf16[1280,2560]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.968 = bf16[1280,2560]{1,0:T(8,128)(2,1)} convert(f32[1280,2560]{1,0:T(8,128)} %get-tuple-element.487)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  17. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.969 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.488)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  18. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1131 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.650)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  19. Size: 6.25M\n",
            "     Shape: bf16[1280,2560]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1130 = bf16[1280,2560]{1,0:T(8,128)(2,1)} convert(f32[1280,2560]{1,0:T(8,128)} %get-tuple-element.649)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  20. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1125 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.644)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "\n",
            "\tTPU compilation failed\n",
            "\t [[{{node TPUReplicateMetadata}}]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\t [[tpu_compile_succeeded_assert/_1398667662751940758/_4_G3286]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "  (1) Resource exhausted: Compilation failure: Ran out of memory in memory space hbm. Used 8.29G of 7.48G hbm. Exceeded hbm capacity by 824.03M.\n",
            "\n",
            "Total hbm usage >= 8.80G:\n",
            "    reserved        530.00M \n",
            "    program           6.86G \n",
            "    arguments         1.42G (100.0% utilization)\n",
            "\n",
            "Output size 5.0K (80.1% utilization); shares 0B with arguments.\n",
            "\n",
            "Program hbm requirement 6.86G:\n",
            "    global           13.16M\n",
            "    scoped            2.51M\n",
            "    HLO temp          6.85G (100.0% utilization: Unpadded (6.85G) Padded (6.85G), 0.0% fragmentation (2.01M))\n",
            "\n",
            "  Largest program allocations in hbm:\n",
            "\n",
            "  1. Size: 3.07G\n",
            "     Operator: op_type=\"CrossReplicaSum\" op_name=\"gpt2/xentropy_final/reduce_logsumexp/reduce_max/CrossReplicaSum\"\n",
            "     Shape: f32[8,2048,50257]{1,2,0:T(8,128)}\n",
            "     Unpadded size: 3.07G\n",
            "     Extra memory due to padding: 448.0K (1.0x expansion)\n",
            "     XLA label: %all-reduce.7419 = f32[8,2048,50257]{1,2,0:T(8,128)} all-reduce(f32[8,2048,\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 257, in <module>\n",
            "    main(args)\n",
            "  File \"main.py\", line 221, in main\n",
            "    run_eval()\n",
            "  File \"main.py\", line 201, in run_eval\n",
            "    steps=params[\"eval_steps\"])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3151, in evaluate\n",
            "    rendezvous.raise_errors()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py\", line 150, in raise_errors\n",
            "    six.reraise(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/six.py\", line 703, in reraise\n",
            "    raise value\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3146, in evaluate\n",
            "    name=name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 467, in evaluate\n",
            "    name=name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 510, in _actual_eval\n",
            "    return _evaluate()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 499, in _evaluate\n",
            "    output_dir=self.eval_dir(name))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1647, in _evaluate_run\n",
            "    config=self._session_config)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/evaluation.py\", line 272, in _evaluate_once\n",
            "    session.run(eval_ops, feed_dict)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 778, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1283, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1384, in run\n",
            "    raise six.reraise(*original_exc_info)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/six.py\", line 703, in reraise\n",
            "    raise value\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1369, in run\n",
            "    return self._sess.run(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1442, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1200, in run\n",
            "    return self._sess.run(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 968, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1191, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1369, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1394, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.ResourceExhaustedError: From /job:worker/replica:0/task:0:\n",
            "9 root error(s) found.\n",
            "  (0) Resource exhausted: Compilation failure: Ran out of memory in memory space hbm. Used 8.29G of 7.48G hbm. Exceeded hbm capacity by 824.03M.\n",
            "\n",
            "Total hbm usage >= 8.80G:\n",
            "    reserved        530.00M \n",
            "    program           6.86G \n",
            "    arguments         1.42G (100.0% utilization)\n",
            "\n",
            "Output size 5.0K (80.1% utilization); shares 0B with arguments.\n",
            "\n",
            "Program hbm requirement 6.86G:\n",
            "    global           13.16M\n",
            "    scoped            2.51M\n",
            "    HLO temp          6.85G (100.0% utilization: Unpadded (6.85G) Padded (6.85G), 0.0% fragmentation (2.01M))\n",
            "\n",
            "  Largest program allocations in hbm:\n",
            "\n",
            "  1. Size: 3.07G\n",
            "     Operator: op_type=\"CrossReplicaSum\" op_name=\"gpt2/xentropy_final/reduce_logsumexp/reduce_max/CrossReplicaSum\"\n",
            "     Shape: f32[8,2048,50257]{1,2,0:T(8,128)}\n",
            "     Unpadded size: 3.07G\n",
            "     Extra memory due to padding: 448.0K (1.0x expansion)\n",
            "     XLA label: %all-reduce.7419 = f32[8,2048,50257]{1,2,0:T(8,128)} all-reduce(f32[8,2048,50257]{1,2,0:T(8,128)} %fusion.364), replica_groups={{0,1},{2,3},{4,5},{6,7}}, to_apply=%sum.2597, metadata={op_type=\"CrossReplicaSum\" op_name=\"gpt2/xentropy_final/reduce_logsumexp/...\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  2. Size: 3.07G\n",
            "     Operator: op_type=\"Einsum\" op_name=\"gpt2/wte_final_einsum/einsum/einsum/Einsum\"\n",
            "     Shape: f32[8,2048,50257]{1,2,0:T(8,128)}\n",
            "     Unpadded size: 3.07G\n",
            "     Extra memory due to padding: 448.0K (1.0x expansion)\n",
            "     XLA label: %fusion.364 = f32[8,2048,50257]{1,2,0:T(8,128)} fusion(bf16[50257,1280]{1,0:T(8,128)(2,1)} %get-tuple-element.38954, f32[1280]{0:T(1024)} %get-tuple-element.37548, f32[1280]{0:T(1024)} %get-tuple-element.37547, f32[8,2048]{1,0:T(8,128)} %fusion.2250, f32[8...\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  3. Size: 122.71M\n",
            "     Shape: bf16[50257,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 122.70M\n",
            "     Extra memory due to padding: 17.5K (1.0x expansion)\n",
            "     XLA label: %convert.1151 = bf16[50257,1280]{1,0:T(8,128)(2,1)} convert(f32[50257,1280]{1,0:T(8,128)} %get-tuple-element.670)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  4. Size: 8.00M\n",
            "     Shape: s32[2048,1024]{0,1:T(8,128)}\n",
            "     Unpadded size: 8.00M\n",
            "     XLA label: constant literal\n",
            "     Allocation type: global\n",
            "     ==========================\n",
            "\n",
            "  5. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1023 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.542)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  6. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1095 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.614)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  7. Size: 6.25M\n",
            "     Shape: bf16[1280,2560]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1022 = bf16[1280,2560]{1,0:T(8,128)(2,1)} convert(f32[1280,2560]{1,0:T(8,128)} %get-tuple-element.541)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  8. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1149 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.668)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  9. Size: 6.25M\n",
            "     Shape: bf16[1280,2560]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1148 = bf16[1280,2560]{1,0:T(8,128)(2,1)} convert(f32[1280,2560]{1,0:T(8,128)} %get-tuple-element.667)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  10. Size: 6.25M\n",
            "     Shape: bf16[1280,2560]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.962 = bf16[1280,2560]{1,0:T(8,128)(2,1)} convert(f32[1280,2560]{1,0:T(8,128)} %get-tuple-element.481)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  11. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.963 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.482)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  12. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1143 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.662)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  13. Size: 6.25M\n",
            "     Shape: bf16[1280,2560]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1142 = bf16[1280,2560]{1,0:T(8,128)(2,1)} convert(f32[1280,2560]{1,0:T(8,128)} %get-tuple-element.661)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  14. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1137 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.656)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  15. Size: 6.25M\n",
            "     Shape: bf16[1280,2560]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1136 = bf16[1280,2560]{1,0:T(8,128)(2,1)} convert(f32[1280,2560]{1,0:T(8,128)} %get-tuple-element.655)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  16. Size: 6.25M\n",
            "     Shape: bf16[1280,2560]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.968 = bf16[1280,2560]{1,0:T(8,128)(2,1)} convert(f32[1280,2560]{1,0:T(8,128)} %get-tuple-element.487)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  17. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.969 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.488)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  18. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1131 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.650)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  19. Size: 6.25M\n",
            "     Shape: bf16[1280,2560]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1130 = bf16[1280,2560]{1,0:T(8,128)(2,1)} convert(f32[1280,2560]{1,0:T(8,128)} %get-tuple-element.649)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "  20. Size: 6.25M\n",
            "     Shape: bf16[2560,1280]{1,0:T(8,128)(2,1)}\n",
            "     Unpadded size: 6.25M\n",
            "     XLA label: %convert.1125 = bf16[2560,1280]{1,0:T(8,128)(2,1)} convert(f32[2560,1280]{1,0:T(8,128)} %get-tuple-element.644)\n",
            "     Allocation type: HLO temp\n",
            "     ==========================\n",
            "\n",
            "\n",
            "\tTPU compilation failed\n",
            "\t [[node TPUReplicateMetadata (defined at /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3667) ]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "\t [[tpu_compile_succeeded_assert/_1398667662751940758/_4_G3286]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\n",
            "  (1) Resource exhausted: Compilation failure: Ran out of memory in memory space hbm. Used 8.29G of 7.48G hbm. Exceeded hbm capacity by 824.03M.\n",
            "\n",
            "Total hbm usage >= 8.80G:\n",
            "    reserved        530.00M \n",
            "    program           6.86G \n",
            "    arguments         1.42G (100.0% utilization)\n",
            "\n",
            "Output size 5.0K (80.1% utilization); shares 0B with arguments.\n",
            "\n",
            "Program hbm requirement 6.86G:\n",
            "    global           13.16M\n",
            "    scoped            2.51M\n",
            "    HLO temp          6.85G (100.0% utilization: Unpadded (6.85G) Padded (6.85G), 0.0% fragmentation (2.01M))\n",
            "\n",
            "  Largest program allocations in hbm:\n",
            "\n",
            "  1. Size: 3.07G\n",
            "     Operator: op_type=\"CrossReplicaSum\" op_name=\"gpt2/xentropy_final/reduce_logsumexp/reduce_max/CrossReplicaSum\"\n",
            "     Shape: f32[8,2048,50257]{1,2,0:T(8,128)}\n",
            "     Unpadded size: 3.07G\n",
            "     Extra memory due to padding: 448.0K (1.0x expansion)\n",
            "     XLA label: %all-reduce.7419 = f32[8,2048,50257]{1,2,0:T(8,128)} all-reduce(f32[8,2048,\n",
            "\n",
            "Original stack trace for 'TPUReplicateMetadata':\n",
            "  File \"main.py\", line 257, in <module>\n",
            "    main(args)\n",
            "  File \"main.py\", line 221, in main\n",
            "    run_eval()\n",
            "  File \"main.py\", line 201, in run_eval\n",
            "    steps=params[\"eval_steps\"])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3146, in evaluate\n",
            "    name=name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 467, in evaluate\n",
            "    name=name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 510, in _actual_eval\n",
            "    return _evaluate()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 492, in _evaluate\n",
            "    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1531, in _evaluate_build_graph\n",
            "    self._call_model_fn_eval(input_fn, self.config))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1567, in _call_model_fn_eval\n",
            "    config)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 2962, in _call_model_fn\n",
            "    config)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1163, in _call_model_fn\n",
            "    model_fn_results = self._model_fn(features=features, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3401, in _model_fn\n",
            "    _eval_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3667, in _eval_on_tpu_system\n",
            "    device_assignment=ctx.device_assignment)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/tpu/tpu.py\", line 1749, in split_compile_and_shard\n",
            "    xla_options=xla_options)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/tpu/tpu.py\", line 1374, in split_compile_and_replicate\n",
            "    num_replicas=num_replicas, use_tpu=use_tpu, **metadata_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_tpu_ops.py\", line 6599, in tpu_replicate_metadata\n",
            "    name=name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 750, in _apply_op_helper\n",
            "    attrs=attr_protos, op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 3536, in _create_op_internal\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 1990, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTJL6QdQ9Kl2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}